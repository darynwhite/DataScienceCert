{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49fc0dfe",
   "metadata": {},
   "source": [
    "# Classification Accuracy Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use minimal packages to expose and explain accuracy measures\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76382832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a simple and small data to keep calculations understandable\n",
    "# data to work with (T = target labels, y = prob. output of classifier, pred = closest class predictions)\n",
    "T = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "y = np.array([0,0.15,0.1,0.9,0.8,0.02,0.79,0.75,0.55,0.25,0.15,0.12,0.17,0.25,0.19,0.06,0.97,0.13,0.25,0.04,0.03,0.02,0.07,0.08,0.96,0.26,0.24,0.22,0.09,0.96,0.91,0.08,0.94,0.84,0.45,0.35,0.9,0.85,0.75,0.72,0.89,.75,0.4,0.91,0.42,0.44,0.89,0.86,0.83,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf70769",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    " The most common confusion matrix is a 2-by-2 matrix that contains the following 4 numbers:  \n",
    "- True Positive (TP)\n",
    "- True Negative (TN)\n",
    "- False Positive (FP)\n",
    "- False Negative (FN).  \n",
    "\n",
    "<br/>The correct (True) predictions are on the diagonal from the upper right to the lower left.  True Positive is sometimes placed in the upper left and sometimes in the lower right of the matrix.  We will place True Positive in lower right to be consistent with sklearn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method creates a confusion matrix\n",
    "# method is designed to return the same result as sklearn.metrics.confusion_matrix\n",
    "# Confusion matrix:\n",
    "# [[TN FP]\n",
    "#  [FN TP]]\n",
    "# TP is True Positive; TN is True Negative; FP is False Positive; FN is False Negative\n",
    "def confmat(actual, predicted):\n",
    "    YES = max(max(actual), max(predicted))\n",
    "    true_positive  = sum((predicted == actual) & (predicted == YES))\n",
    "    true_negative  = sum((predicted == actual) & (predicted != YES))\n",
    "    false_positive = sum((predicted != actual) & (predicted == YES))\n",
    "    false_negative = sum((predicted != actual) & (predicted != YES))\n",
    "    actual_negative = [true_negative, false_positive]\n",
    "    actual_positive = [false_negative, true_positive]\n",
    "    CM = [actual_negative, actual_positive]\n",
    "    return np.array(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc6af1",
   "metadata": {},
   "source": [
    "To calculate a confusion matrix we need to threshold the target probabilites (`y`).  The typical threshold values is 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold y at 0.5\n",
    "Y = np.round(y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "# https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "CM = confmat(T, Y) # sklearn.metrics.confusion_matrix\n",
    "print (\"Confusion matrix:\")\n",
    "print (CM)\n",
    "tn, fp, fn, tp = CM.ravel()\n",
    "print (\"TP:{}; TN:{},; FP:{}; FN:{}\".format(tp, tn, fp, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6366bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "conmat=confusion_matrix(T, Y)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=conmat, display_labels=[\"0\", \"1\"])\n",
    "font = {'weight': 'bold', 'size': 24}\n",
    "plt.rc('font', **font)\n",
    "cmd.plot();\n",
    "plt.rcdefaults();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23078949",
   "metadata": {},
   "source": [
    "#### Accuracy measures from the confusion matrix\n",
    "The four numbers in a confusion matrix are used to calculate many accuracy measures:  \n",
    "- Accuracy (Accuracy rate)\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "\n",
    "<br/>We cannot calculate an ROC from a single confusion matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = (tp + tn)/(tp + fp + fn + tn) # sklearn.metrics.accuracy_score(T, Y)\n",
    "print (\"\\nAccuracy (Accuracy rate):\", np.round(Accuracy, 2))\n",
    "Precision = tp/(tp + fp) # sklearn.metrics.precision_score(T, Y)\n",
    "print (\"Precision:\", np.round(Precision, 2))\n",
    "Recall = tp/(tp + fn) # sklearn.metrics.recall_score(T, Y)\n",
    "print (\"Recall:\", np.round(Recall, 2))\n",
    "F1 = 2./(1./Recall + 1./Precision) # sklearn.metrics.f1_score(T, Y)\n",
    "print (\"F1 score:\", np.round(F1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e690777",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "The gold standard for accuracy in a binary classification is the ROC curve.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method returns the data for an ROC curve\n",
    "# method is designed to emulate sklearn.metrics.roc_curve\n",
    "# method returns superflous thresholds as compared to sklearn.metrics.roc_curve\n",
    "# input parameters\n",
    "#    actual  the actual observations.  Must be binary.  Typically 1/0 or True/False\n",
    "#    predicted_probabilities   the probabilities from a predicted model in range [0, 1]\n",
    "# output parameters as a tuple (roc_values)\n",
    "#    false_positive_rate is the proportion of false positives among all positives\n",
    "#    true_positive_rate is the proportion of true positives among all positives\n",
    "#    thresholds are the values used to threshold the confusion matrix\n",
    "def roc(actual, predicted_probabilities):\n",
    "    thresholds = np.sort(np.unique(np.append(predicted_probabilities, [-1, 1])))\n",
    "    thresholds.sort()\n",
    "    thresholds\n",
    "    false_positive_rate = []\n",
    "    true_positive_rate = []\n",
    "    for threshold in thresholds:\n",
    "        CM = confmat(actual, predicted_probabilities > threshold)\n",
    "        true_negative, false_positive, false_negative, true_positive = CM.ravel()\n",
    "        true_positive_rate.append(true_positive/(true_positive + false_negative))\n",
    "        false_positive_rate.append(false_positive/(false_positive + true_negative))\n",
    "    roc_values = (false_positive_rate, true_positive_rate, thresholds)\n",
    "    return(roc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adedee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display roc curve\n",
    "# has dependency on auc method from sklearn\n",
    "def roc_display(false_positive_rate, true_positive_rate):\n",
    "    from sklearn.metrics import auc\n",
    "    AUC = auc(false_positive_rate, true_positive_rate)\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.grid()\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.xlabel('FALSE Positive Rate')\n",
    "    plt.ylabel('TRUE Positive Rate')\n",
    "    plt.plot([0, 1], [0, 1], color='gold', lw=2, linestyle='--') # reference line for random classifier\n",
    "    plt.plot(false_positive_rate, true_positive_rate, color='purple',lw=2, label='ROC curve (area = %0.2f)' % AUC)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac428c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of ROC\n",
    "false_positive_rate, true_positive_rate, thresholds = roc(T, y)\n",
    "roc_display(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC calculations with sklearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "fpr, tpr, th = roc_curve(y_true = T, y_score = y) # False Positive Rate, True Posisive Rate, probability thresholds\n",
    "auc_of_roc = round(roc_auc_score(y_true = T, y_score = y), 2)\n",
    "RocCurveDisplay(fpr = fpr, tpr = tpr, roc_auc = auc_of_roc).plot()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a749872",
   "metadata": {},
   "source": [
    "### Switch our definition of positive and negative\n",
    "The deinition of positive or negativce is arbitrary.  When we are looking for a legitimate credit card transaction, then we define positive as a legitimate transaction and negative as a fraudulent transaction.  When we are looking for credit card fraud, then define positive as a fraudulent transaction and negative as a legitimate transaction.  We can use the same classification model in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch positive with negative\n",
    "T_neg = 1 - T\n",
    "y_neg = 1 - y\n",
    "Y_neg = 1 - Y # np.round(y_neg, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ddde9",
   "metadata": {},
   "source": [
    "We will show the previous results of the 4 accuracy measures from our confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ff7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous results shown again:\n",
    "print(\"Prevous results:\")\n",
    "print (\"   Accuracy (Accuracy rate):\", np.round(Accuracy, 2))\n",
    "print (\"   Precision:\", np.round(Precision, 2))\n",
    "print (\"   Recall:\", np.round(Recall, 2))\n",
    "print (\"   F1:\", np.round(F1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "# https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "CM = confmat(T_neg, Y_neg) # sklearn.metrics.confusion_matrix\n",
    "print (\"Confusion matrix:\")\n",
    "print (CM)\n",
    "tn, fp, fn, tp = CM.ravel()\n",
    "print (\"TP:{}; TN:{},; FP:{}; FN:{}\".format(tp, tn, fp, fn))\n",
    "Accuracy = (tp + tn)/(tp + fp + fn + tn) # sklearn.metrics.accuracy_score(T, Y)\n",
    "print (\"\\nAccuracy (Accuracy rate):\", np.round(Accuracy, 2))\n",
    "Precision = tp/(tp + fp) # sklearn.metrics.precision_score(T, Y)\n",
    "print (\"Precision:\", np.round(Precision, 2))\n",
    "Recall = tp/(tp + fn) # sklearn.metrics.recall_score(T, Y)\n",
    "print (\"Recall:\", np.round(Recall, 2))\n",
    "F1 = 2./(1./Recall + 1./Precision) # sklearn.metrics.f1_score(T, Y)\n",
    "print (\"F1 score:\", np.round(F1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05236a30",
   "metadata": {},
   "source": [
    "**Important:  When we switch our definition, then precision, recall, and F1, have different results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6b770",
   "metadata": {},
   "source": [
    "#### Compare ROC curves before and after changing the definition of positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b66b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of ROC for positive case\n",
    "false_positive_rate, true_positive_rate, thresholds = roc(T, y)\n",
    "roc_display(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032fd76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of ROC for negative case\n",
    "false_positive_rate, true_positive_rate, thresholds = roc(T_neg, y_neg)\n",
    "roc_display(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3122dad9",
   "metadata": {},
   "source": [
    "**Important:  When we switch our definition, then the AUC of the ROC is the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf992b",
   "metadata": {},
   "source": [
    "The ROC has another very important advantage over a single confusion matrix.  The ROC evaulates a model's full range of probabilities.  Sometimes bad models can have high accuracy scores if their probabilities are all near 0.5.  Conversely, some useable models can have very bad accuracy scores.  The ROC and it's AUC take these effects into account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
