{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification with un-balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we learn the problems we run into when training a classifier to predict rare events. Many binary classification problems involve rare events, such as predicting that someone has a rare disease, or predicting that someone looking at an add will buy the product. When one of the classes (by convention the positive class) makes up only a very small percentage of the data and most data points belong to the other class (by convention the negative class), we call the uncommon class a **rare event** and we say that we have **unbalanced data**. We explore in this notebook how to unbalanced data affects the way that we evaluate the model.\n",
    "\n",
    "The Ames housing dataset has housing data including sale price. We create a binary label to flag houses in a price range and build a classifier to predict the likelihood of this price range for a house.  \n",
    "\n",
    "https://www.kaggle.com/datasets/prevek18/ames-housing-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "house_prices = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "print('house_prices keys:', house_prices.keys())\n",
    "ames_df = house_prices.frame # The value for the 'frame' key is a data frame with features and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "The Ames housing data has missing data. To streamline our analysis, we will eliminate columns with more than 0.10% missing data. Let's first check which columns have `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape:', ames_df.shape)\n",
    "\n",
    "# Checking the columns with NaN\n",
    "print(ames_df.isna().sum().sort_values(ascending=False).to_string())\n",
    "# print(ames_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = ames_df.columns\n",
    "ames_df.loc[ames_df['Electrical'].isna(), 'Electrical'] = 'SBrkr' # this is the most frequent value on this column\n",
    "#\n",
    "ames_df.dropna(axis = 1, inplace = True)\n",
    "print(original_columns.shape[0], 'columns reduced to', ames_df.columns.shape[0],'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = ames_df.columns\n",
    "object_columns_before = ames_df.select_dtypes(include='object').columns\n",
    "numeric_columns = ames_df.select_dtypes(include=np.number).columns\n",
    "print('The data frame has', original_columns.shape[0],\n",
    "      'columns, including', object_columns_before.shape[0],'categorical columns and',\n",
    "      numeric_columns.shape[0], 'numeric columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (4 minutes)\n",
    "\n",
    "- As one example, recall that earlier in the notebook we used `np.unique(...)` to get counts. Use it to get counts for each unique value of a categorical (object) column with more than 3 values.\n",
    "- Also turn the counts into percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since getting counts and turning them into percentages is such a common data-related task, there's got to be an easier way to do it. And there is. Search online to see if `pandas` offers a function for getting unique counts for a column in the data.  Use the pandas method to get the percentages of the categories for `Exterior2nd` and `Neighborhood`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Categorical Complexity\n",
    "- Category columns will need to be one-hot encoded\n",
    "- Too many categories will lead to too many feature dimensions\n",
    "- For simplicity we will drop category columns with too many categories.  Alternatively, we could bin or consolidate similar categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop category columns with more than 5 categories\n",
    "column_drop_list = []\n",
    "for column in object_columns_before:\n",
    "    if np.unique(ames_df[column]).shape[0] > 5:\n",
    "        column_drop_list = column_drop_list + [column]\n",
    "ames_df.drop(columns=column_drop_list, inplace=True)\n",
    "object_columns_after = ames_df.select_dtypes(include='object').columns\n",
    "print(object_columns_before.shape[0], 'category columns reduced to', object_columns_after.shape[0],'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = ames_df.columns\n",
    "ames_df_num = ames_df[numeric_columns].copy() # ames_df.select_dtypes(include=np.number).copy() # only select columns that are numeric\n",
    "ames_df_cat = ames_df[object_columns_after].copy() # ames_df.select_dtypes('object').copy() # only select columns that have type 'object'\n",
    "onehot = OneHotEncoder(sparse = False) # initialize one-hot-encoder\n",
    "onehot.fit(ames_df_cat)\n",
    "col_names = onehot.get_feature_names_out(ames_df_cat.columns) # this allows us to properly name columns\n",
    "ames_df_onehot =  pd.DataFrame(onehot.transform(ames_df_cat), columns = col_names)\n",
    "AmesFeatures = ames_df_num.join(ames_df_onehot)\n",
    "print(object_columns_after.shape[0], 'categry columns expanded to', ames_df_onehot.columns.shape[0],'columns')\n",
    "print('Total of', AmesFeatures.columns.shape[0], 'columns')\n",
    "# AmesFeatures.to_csv('AmesFeatures.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the target variable, housing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(AmesFeatures.SalePrice)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(np.quantile(AmesFeatures.SalePrice, [0.05, 0.1, 0.5, 0.9, 0.95]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we're interested in training a classification algorithm to predict whether or not a house is within a specified price range. So first we create a target variable that flags houses who sold in that price range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (AmesFeatures['SalePrice'] > 200000) & (AmesFeatures['SalePrice'] < 230000)\n",
    "X = AmesFeatures.drop(columns=['SalePrice'])\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by splitting `X` and `y` into a training data and a testing data. The easiest way to do this is using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (6 minutes)\n",
    "\n",
    "- Find counts for the target in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a logistic regression classifier to predict whether the price of a house is within our specified range. Begin by loading the library as such: `from sklearn.linear_model import LogisticRegression`. Then create an instance of the algorithm and train it by invoking the `.fit(X_train, y_train)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once the model is trained, pass it the testing data to see if we get predictions back. To do so, we invoke the `.predict(x_test)` method. We can also invoke the `.predict_proba(x_test)` method if we wish to get the raw probabilites instead of the final predictions.  Name the predictions `y_test_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the accuracy of the model by loading `from sklearn.metrics import accuracy_score` and calling the `accuracy_score` function. What two arguments do we pass to this function to evaluate the model's accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Add code here 006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is accuracy a good metric for evaluating this model? Why or why not? To give some context, let's say you're a developer and want to predict house prices. You prefer to bid low and lose a bid than bid high for a house that's not worth it.\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find some more useful evaluation metrics. The most direct metric to look at, is the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "cm_train = metrics.confusion_matrix(y_train, y_train_pred)\n",
    "print('Confusion matrix based on training data:')\n",
    "print(cm_train)\n",
    "print('\\nConfusion matrix as an accuracy measure (test data):')\n",
    "cm_test = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, we can derive accuracy, precision, recall, and the F1-score, which is a sort of average of precision and recall. We don't have time to get into all of them in detail, but [here](http://www.win-vector.com/blog/2009/11/i-dont-think-that-means-what-you-think-it-means-statistics-to-english-translation-part-1-accuracy-measures/) is an excellent article explaining in great detail the differences between each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC:  Gold Standard for Classification Accuracy\n",
    "\n",
    "One way to visually evaluate a binary classification model is using the ROC plot. The ROC plot is the only metric that can be used by itself and that has some meaning across different data sets.  The area under the ROC plot is called AUC (area under the curve) and the closer it is to 1, the better the model.  An AUC of 0.5 is a random model and is considered to be the worst case scenario.  Such a model is represented by the diagonal from the lower left to the upper right.  (A model that performs below 0.5 with confidence just needs its labels reversed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_proba)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_proba)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above ROC plot for the test data we see a plot whose ROC curve is somewhat above the diagonal.  This means that the probabilities are not very useful.  The AUC is around 0.61 and the scale is from 0.5 to 1.  This AUC indicates that the model is very weak. \n",
    "\n",
    "#### Basics on AUC\n",
    "- For a first understanding of AUC scores we can presume the following (obviously every situation is different and determined by the business use case):\n",
    "    - A random binary classification model (like a flip of a coin) is 50% (worst accuracy)\n",
    "    - A barely useable model has an AUC above 0.72. \n",
    "    - A good model has an AUC above 0.8.\n",
    "    - A very good model has an AUC above 0.85\n",
    "    - A model with an AUC above 0.95 is often an indicator for target leakage (*too good to be true*)\n",
    "- With the ROC we do not have to worry that the AUC is different for the positive case vs. the negative case as it is for recall, precision and F1 score.  Each of recall, precision, and F1 scores require 2 values, namely one for the positive case and one for the negative case.  The ROCs of a positive case is the mirror image of a negative case.  The AUC of the ROC is the same regardless of which binary outcome is selected as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Class Imbalance\n",
    "Now that we have seen how to different approaches to evaluating machine learning models. Let's take a look at solving class imbalance problem. We need to alway make sure to apply downsampling and upsampling to training data only.\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-sampling\n",
    "- An unbalanced data set is where the classes have different number of cases/observations/rows.  \n",
    "- The class with the most observations is called the majority class.  \n",
    "- The class with the least observations is called the minority class.  \n",
    "- Down-sampling is a technique where the majority class is reduced in size to match the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfRowsPerClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The imbalanced class distribution for the train data was {}:{}'\n",
    "      .format(y_train.shape[0] - np.sum(y_train), np.sum(y_train)))\n",
    "# Extract positive cases\n",
    "X_train_pos = X_train[y_train == 1]  # X_train_pos = X_train.loc[y_train, :]\n",
    "numberOfRowsPerClass = X_train_pos.shape[0] # np.sum(y_train)\n",
    "y_train_pos = y_train[y_train == 1]  # y_train_pos = np.ones(numberOfRowsPerClass, dtype=bool)\n",
    "# Extract negative cases\n",
    "X_train_neg = X_train[y_train == 0]  # X_train_neg = X_train.loc[~y_train, :]\n",
    "y_train_neg = y_train[y_train == 0]\n",
    "# Down-sample negative cases\n",
    "X_train_neg_downsample = X_train_neg.sample(n=numberOfRowsPerClass, random_state=0)\n",
    "y_train_neg_downsample = y_train_neg.sample(n=numberOfRowsPerClass)\n",
    "\n",
    "# combine positive and negative cases\n",
    "X_train_downsample = pd.concat([X_train_pos, X_train_neg_downsample], ignore_index=True)\n",
    "y_train_downsample = np.concatenate([y_train_pos, y_train_neg_downsample])\n",
    "\n",
    "print('The balanced class distribution for the down-sampled train data was {}:{}'\n",
    "      .format(y_train_neg_downsample.shape[0], y_train_pos.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregDownSample = LogisticRegression()\n",
    "logregDownSample.fit(X_train_downsample, y_train_downsample)\n",
    "\n",
    "y_train_down_pred = logregDownSample.predict(X_train_downsample)\n",
    "y_train_down_proba = logregDownSample.predict_proba(X_train_downsample)[:,1]\n",
    "\n",
    "y_test_down_pred = logregDownSample.predict(X_test)\n",
    "y_test_down_proba = logregDownSample.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('Training \"accuracy\":', accuracy_score(y_true=y_train_downsample, y_pred=y_train_down_pred))\n",
    "print('Accuracy (test data):', accuracy_score(y_true=y_test, y_pred=y_test_down_pred))\n",
    "\n",
    "cm_down_train = metrics.confusion_matrix(y_train_downsample, y_train_down_pred)\n",
    "print('Confusion matrix based on down-sampled training data:')\n",
    "print(cm_down_train)\n",
    "print('\\nConfusion matrix as an accuracy measure (test data):')\n",
    "cm_down_test = metrics.confusion_matrix(y_test, y_test_down_pred)\n",
    "print(cm_down_test)\n",
    "\n",
    "print(classification_report(y_train_downsample, y_train_down_pred))\n",
    "print(classification_report(y_test, y_test_down_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_train_downsample, y_train_down_proba)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_down_proba)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling (Data Augmentation)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Data_augmentation\n",
    "\n",
    "Note that **SMOTE** stands for **Synthetic Minority Over-sampling Technique**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_upsample = LogisticRegression(max_iter=5000)\n",
    "logit_upsample.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat_augment = logit_upsample.predict(X_resampled)\n",
    "print('Training accuracy score:', accuracy_score(y_true=y_resampled, y_pred=y_train_hat_augment))\n",
    "cm = metrics.confusion_matrix(y_resampled, y_train_hat_augment)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_upsample_hat_pred = logit_upsample.predict(X_test)\n",
    "print('Accuracy score (based on test data):', accuracy_score(y_true=y_test, y_pred=y_test_upsample_hat_pred))\n",
    "cm = metrics.confusion_matrix(y_test, y_test_upsample_hat_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_upsample_hat_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat_augment_proba = logit_upsample.predict_proba(X_resampled)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_resampled, y_train_hat_augment_proba)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_augment_proba = logit_upsample.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_hat_augment_proba)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
