{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we talk about some of the functions and methods that are commonly used to deal with numeric and categorical columns in our data in preparation to doing machine learning. Without properly pre-processing the data, our ML algorithm will either fail to execute or execute but return results that are incorrect or incomplete. Since we haven't introduced machine learning yet, it's too early to show examples of how this can happen, but we will see examples in future lectures.\n",
    "\n",
    "As usual, let's begin by reading some data. We want to read the data and find some ways to summarize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# you need to run this to produce visualizations in a jupyter notebook\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.precision = 3\n",
    "\n",
    "LocalFile = '../../data/auto-mpg.csv'\n",
    "UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "url = UCI_url\n",
    "auto = pd.read_csv(url, sep = '\\s+', header = None, \n",
    "                   names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', \n",
    "                            'acceleration', 'model year', 'origin', 'car_name'])\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataframe size and column data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(auto.shape)\n",
    "display(auto.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have quite a few numeric columns in here. Numeric columns can be **integer**, **float**, or **datetime** columns. However, we also need to be careful: a column like `cylinders` looks numeric, but probably should be treated as a categorical column. Why? Ask yourself which one of these two is more likely:\n",
    "\n",
    "- do any kind of math with `cylinders`, such as taking the log of it, adding it to another column, or being interested in knowing the average number of cylinders?\n",
    "- use cylinder as a grouping variable, such as wanting to see the average horsepower **by cylinder**?\n",
    "\n",
    "We can probably agree that the second case is more likely. For this reason, we need to think of these types of variables as categorical despite them having numeric values.\n",
    "\n",
    "Let's now look at some distributions for our numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(auto.sample(100),  kind='scatter', plot_kws={'alpha':0.25});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the feature seem to have any extreme values. Let's check for missing values. To see if any columns in the data are missing, we can use the `isnull` method, followed by `any` where the `axis = 0` is used to ask if **any** of values **across columns** is null: `False` means that none is missing, and `True` means that at least one value is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.isnull().any(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better would be to use `sum` to get the count of values that are missing for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of this would work only if the missing data is properly marked as null. Often when reading external data sources, other conventions are used to mark data as missing, such as using `-999` for numeric data, or the string `NA` or `?` or the empty string for categorical data. Such values by default would not be picked up by `isnull`. So we would need to first recode them as `np.nan`.\n",
    "\n",
    "### Exercise (8 minutes)\n",
    "\n",
    "Let's illustrate the above point with an example, let's intentially introduce a few missing values in the data, but using characters that won't be recognized as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.loc[2, ['mpg', 'car_name']] = [-999, \"\"] # make mpg and car_name in the 3rd row NA\n",
    "# Add zero to origin of 4th car\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look for the following occurences to detect the missing values:\n",
    "- inappropriate data types\n",
    "- the length of strings in `car_name`\n",
    "- outliers in numerical values\n",
    "- A numerical value of zero (0) can be a stand-in for null.\n",
    "- very common or very rare values:  Check number of unique values and if reasonable, review the values\n",
    "- Unusual characters like question mark (\"?\"), blank (\" \"), empty (\"\"), \"x\", \"-\"\n",
    "  \n",
    "How can we use a boolean indexer in `.loc` of `pandas` to find the missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 001\n",
    "auto.loc[auto['mpg'] < 0,'mpg'], auto.loc[auto['car_name'] == \"\",'car_name']\n",
    "# for row in auto.iterrows():\n",
    "#     if row[1] < 0:\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in auto.dtypes.items():\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col,type in auto.dtypes.items() if type != 'object']\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(auto.select_dtypes(['int64','float64']) < 0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace the above missing values with `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 002\n",
    "auto.loc[auto['mpg'] < 0,'mpg'] = np.nan\n",
    "auto.loc[auto['car_name'] == \"\",'car_name'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now show any rows in the data that have any missing values. HINT: you will need to use `any` with `axis = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 003\n",
    "auto.loc[auto.isnull().any(axis=1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can see from the scatter plot matrix that `weight` and `displacement` have a **positive relationship**.  A positive relationship means that as one goes up, so does the other. We may wonder if `cylinders` plays a part.\n",
    "\n",
    "- Plot the scatter plot between `weight` and `displacement`, color-coded by `cylinders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Obtain some basic summary statistics for the columns in `auto`. HINT: Use `describe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result of `describe` itself looks looks like a `pandas` `DataFrame` or `Series`. That means that if we wanted to extract certain pieces out of it, we should be able to do it using the methods we learned for working with `DataFrame` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract the mean and standard deviation for acceleration from the results above and store them in variables called `acc_mean` and `acc_std`. Then print a statement that states what the mean and standard deviations are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise\n",
    "\n",
    "There are other functions we can use to get additional summary statistics from the data. For example, notice that `describe` only shows us the 25th, 50th, and 75th **percentiles**. The $p$th percentile is **a value such that $p$ percent of the data is below that value**, which means the remaining $1-p$ percent of the data is above that value. For example, if the 25th percentile for acceleration `mpg` is 17.5, then 25 percent of cars in our data have `mpg` below 17.0. The 50th percentile is also known as the **median**.\n",
    "\n",
    "Looking at percentiles helps us get a feel for the distribution of the data. For example, looking at very low or very high percentiles can help us identify **outliers** or **extreme values**. We can use the `quantile` method to get any percentile we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.mpg.quantile(q = [0, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can obtain these same summary statistics, but grouped by `cylinders`. There is already one way we know how to do that: we can first subset the data by `cylinders == 3` and run the summary statistic, then do the same for `cylinders == 4`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.loc[auto['cylinders'] == 3, :].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But of course the above way is tedious. Instead, we can use the `groupby` method to do it all at once. For example, here's the average of each column grouped by `cylinders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.groupby('cylinders').mpg.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly confusing thing happens if we use `describe` instead of `mean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = auto.groupby('cylinders').describe()\n",
    "print(\"dataframe with multi-level columns:\")\n",
    "display(results_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the above table is that there's a lot of information packed in it. Each value of `cylinders` has its own row, which is fine. But the columns look like they have a hierarchical structure, which makes sense because we asked for many summary statistics for each column.\n",
    "\n",
    "A pandas series is a 1-D collection of scalars.  A typical pandas dataframe is a 2-D collection of scalars.  `results_all` is a multi-level dataframe, which is a 3D collection of scalars.  The 3 Dimensions are:\n",
    "1. Cylinder Values: [3, 4, 5, 6, 8]\n",
    "2. Numeric Columns (except Cylinder): [mpg, displacement, weight, acceleration, model, year, origin]\n",
    "3. Descriptive Statistics:  [count, mean, std, min, 25%, 50%, 75%, max]\n",
    "\n",
    "So what if for example we wanted to look at the results for `acceleration` only. Turns out we can index the multi-level dataframe the same way we usually do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"One slice of a multi-level dataframe is a typical dataframe:\")\n",
    "display(results_all['acceleration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference with before is that `results_all['acceleration']` isn't just a single column, but is itself a `DataFrame`. That's because `results_all` has hierarchical columns, so its columns are themselves `DataFrame` objects. So if we want to drill further down to a specific column, such as `std`, we need to go one level deeper to get the series for the standard deviations of acceleration by cylinder.  We can even drill down one more level and request the standard deviation of acceleration for 3-cylinder vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"series for the standard deviations of acceleration by cylinder:\")\n",
    "display(results_all['acceleration']['std'])\n",
    "print(\"###################\")\n",
    "print(\"scalar for the standard deviation of acceleration for 3-cylinder vehicles:\")\n",
    "display(results_all['acceleration']['std'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far it looks like hierarchical columns are not that complicated. However, things do get a little confusing when we want to cross the hierarchy the other way around. For example, let's say we want to quickly compare the standard deviations of all our columns, grouped by `cylinders`. This information is in the above table, but how do we extract it?\n",
    "\n",
    "It turns out that to do this, we need to use the `IndexSlice` function in `pandas`. Here how it works:\n",
    "- we use `loc` to say which rows and columns we want\n",
    "- we use `slice(None)` to say we want everything at a given hierarchy\n",
    "- if we want to filter at any level of hierarchy, we provide the index we want to filter by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import IndexSlice as idx\n",
    "results_all.loc[: , idx[:, 'std']] # results_all.loc[: , idx[:, ['std', 'mean']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the same way that we can have hierarchical columns, we can also have hierarchical rows, more specifically hierachical row indexes. Examine the result generated by the cell below. Do see the hierarchical structure of the row index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = np.arange(0, 1, 0.1)\n",
    "auto.groupby('cylinders')[['mpg', 'displacement']].quantile(q = percentiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical indexes are called `MultiIndex` in `pandas`. This is a more advanced topic and we will leave it at that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (7 minutes)\n",
    "\n",
    "We saw how a histogram shows us counts for **evenly-sized** intervals of a numeric column. But what if we wanted to see counts for any intervals we specify, evenly-sized or not? Here's an example:\n",
    "\n",
    "- Find out how many cars have `mpg` less than 18, between 18 and 25, and 25 or more? HINT: There are many ways of doing this, so to narrow it down a little do this:\n",
    "  - use `loc` to limit data to the above ranges\n",
    "  - use the `shape` method to get row counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 007\n",
    "autos_under18 = auto[auto.mpg < 18]\n",
    "autos_under25 = auto[(auto.mpg > 18) & (auto.mpg < 25)]\n",
    "autos_over25 = auto[auto.mpg > 25]\n",
    "\n",
    "print(autos_over25.shape,autos_under25.shape,autos_under18.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many rows were not counted because they did not fall into any of these intervals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 008\n",
    "rows_count = autos_over25.shape[0] + autos_under25.shape[0] + autos_under18.shape[0]\n",
    "rows_lost = auto.shape[0] - rows_count\n",
    "\n",
    "print(f\"total rows are {rows_count} compared to {auto.shape[0]} thus we lost {rows_lost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another, much faster way to solve the exercise above: \n",
    "\n",
    "- Use the `pd.cut` function to bin `mpg` and specify `[0, 18, 25, 100]` as the cut-offs (the `bins` argument). Store the results as a new column in the data called `mpg_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 009\n",
    "mpg_cat = pd.cut(auto['mpg'], [0,18,25,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `value_counts` to get counts for `mpg_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 010\n",
    "mpg_cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create `mpg_cat` again, but make the following changes to it: \n",
    "  - Note that the choice of using 0 and 100 as the lower and upper bounds is a little arbitrary. So use `-np.Inf` and `np.Inf` instead.\n",
    "  - By default, cut labels the catgories based on the interval range it covers, but we can change the labels to anything we like using the `labels` argument. Rename the labels to `low`, `med` and `high`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 011\n",
    "mpg_cat = pd.cut(auto.mpg, [-np.inf,18,25,np.inf])\n",
    "mpg_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_cat.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Summarization versus Feature Transformation\n",
    "The `pd.cut` function is different from the other functions (or methods) we learned about so far. The `describe`, `mean`, `quantile`, or `value_counts` functions are **feature summarization** functions, but `pd.cut` is a **feature transformation** function, meaning that not only its input, but also its output are features. In the above example, the input to `pd.cut` was `mpg`, and the output was the `mpg_cat` which we chose to append to the data as a new column, but could have also stored in a separate variable. Storing it as a separate variable is a good idea if the variable stores some intermediate values that we use as part of a transformation but don't need anymore afterwards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with categorical data\n",
    "\n",
    "Let's now look at common tasks for treating **categorical data** prior to modeling. Categorical data needs a lot of attention during data pre-processing. This is because most machine learning algorithms don't deal directly with categorical data. Instead we need to **recode** the data from categorical into numeric, and we will see how we do that in this notebook.\n",
    "\n",
    "Let's begin by reading some data. We will use a marketing data set of bank customers. You can read more about the data [here](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bank = pd.read_csv('../../data/bank-full.csv', sep = \";\")\n",
    "bank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data contains many categorical columns, including the target itself. Let's check the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bank.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `select_dtypes` method to limit the data to just the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank.select_dtypes('object').head() # bank.loc[:, bank.dtypes == 'object'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out there are **two kinds of data types for categorical data** in `pandas`: `object` and `category`. By default, any non-numeric column will inherit the `object` type, but we can later convert it to `category` type. An `object` column type can be modified at will, but a `catogory` type is only appropriate for a column with **a limited number pre-defined categories**. This is because the `category` type is a more rigid data type with additional limitations on what they can store. So this only makes sense when the categories are known and few. Let's illustrate that by turning some of the columns in our data into a `category` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['marital', 'default', 'housing', 'loan']\n",
    "bank[cat_cols] = bank[cat_cols].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to add additional rigidity? Because this way we can impose some amount of **data integrity**. For example, if `marital` should always be limited to \"single\", \"divorced\" or \"married\" then by converting `marital` into a `category` column we can prevent the data from introducing any other category without first adding it as one of the acceptable categories for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank['marital'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (6 minutes)\n",
    "\n",
    "- Try to change the `marital` column at the second row to the value \"widowed\". You should get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Is widowed a category in the marital column?  ', 'widowed' in bank['marital'].cat.categories)\n",
    "# Uncomment the following line of code:\n",
    "# bank.loc[1, 'marital'] = \"widowed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To fix the error, you need to add \"widowed\" as one of the acceptable categories. Use the `cat.add_categories` method to add \"widowed\" as a category and then try again to make sure you don't get an error this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 012\n",
    "bank['marital'] = bank['marital'].cat.add_categories('widowed')\n",
    "bank.loc[1,'marital'] = 'widowed'\n",
    "bank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Undo your change by reassigning the `marital` column at the second row to the value \"single\". Get a count of unique values for `marital` now. Do you notice anything? Explain what and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 013\n",
    "bank.loc[1,'marital'] = 'single'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns have other useful methods, and their names speak for themselves, such as\n",
    "`as_ordered`, `as_unordered`, `remove_categories`, `remove_unused_categories`, `rename_categories`, `reorder_categories`, and `set_categories`. It is important to be aware of this functionality and use it when it makes sense. Of course an alternative to using these is to convert the column back to `object` and make all the changes we want and then turn it back into `category`, but using the above methods makes the code \"cleaner\".\n",
    "\n",
    "So we saw that a `category` column has pre-defined categories and a set of methods specific to itself for changing the categories, whereas an `object` column is more a type of **free-form** column where any value is acceptable. One way the above distinction matters if when we need to rename the categories for a categorical column. Changing the categories of a categorical column is an example of **recoding** or **remapping**.\n",
    "\n",
    "- Let's first begin with an example using `job`, which has type `object`. Rename the category \"management\" to \"managerial\". HINT: find all rows where `job` is the string `'management'`, and use `loc` to change those rows to the string `'managerial'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 014\n",
    "bank.loc[bank.job == 'management', 'job'] = 'managerial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach works fine, but it's tedious if we have a lot of changes we want to make. The better way to do it is to create a Python dictionary that maps old values (values we want to change) to new values, then use the `replace` method to replace them all at once.\n",
    "\n",
    "- Create such a dictionary and use `replace` to make the following changes in the `job` column:\n",
    "\n",
    "  - rename `'student'` to `'in-school'`\n",
    "  - consolidate `'housemaid'` and `'services'` into a single group called `'catering'`\n",
    "  - change `unknown` to a missing value, i.e. `np.NaN` (without quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 015\n",
    "ReplacementMap = {'student':'in-school', 'housemaid':'catering', 'services':'catering', \"unknown\":np.nan}\n",
    "bank.job.replace(ReplacementMap, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above transformation was a renaming and a consolidation (grouping, binning) of catgories!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get a count of unique values for `job` to make sure everything worked. Note that `value_counts()` does not provide count for missing values by default. We need to specify `dropna = False` to include missing vaules in the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 016\n",
    "bank.job.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `replace` method works equally well with a column of type `category`, however **it changes its type to `object`!** So either we have to convert it back to `category`, or we need to use the `rename_categories` method to replace values, which workes very similarly to `replace`. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank['marital'] = bank['marital'].cat.rename_categories({'married': 'taken'})\n",
    "bank['marital'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns can also be easily generated from numeric columns. For example, let's say we want to have a column called `high_balance` that is `True` when balance exceeds $2,000 and `False` otherwise. Technically this would be a boolean column, but in practice it acts as categorical column. Generating such a column is very easy. We refer to such binary colums as **dummy variables** or **flags** because they single out a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank['high_balance'] = bank['balance'] > 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating a dummy variable **for each category** of a categorical feature is called **one-hot encoding**. Let's see what happens if we one-hot-encode `marital`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank['marital_taken'] = (bank['marital'] == 'taken').astype('int')\n",
    "bank['marital_single'] = (bank['marital'] == 'single').astype('int')\n",
    "bank['marital_divorced'] = (bank['marital'] == 'divorced').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank.filter(like = 'marital').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a common enough task that we don't need to do it manually like we did above. Instead we can use `pd.get_dummies` to do it in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(bank[['marital', 'job']], prefix = ['marital', 'job']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an even more streamlined way to do one-hot encoding, although at first blush it appears less straight-forward, but there is a reason it is set up this way and we will explain that later. Just like normalization, one-hot-encoding is a common pre-processing task and we can turn to the `sklearn` library to do the hard part for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "bank_cat = bank.select_dtypes('category').copy() # only select columns that have type 'category'\n",
    "onehot = OneHotEncoder(sparse_output = False) # initialize one-hot-encoder\n",
    "onehot.fit(bank_cat)\n",
    "col_names = onehot.get_feature_names_out(bank_cat.columns) # this allows us to properly name columns\n",
    "bank_onehot =  pd.DataFrame(onehot.transform(bank_cat), columns = col_names)\n",
    "bank_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that one-hot encoding created a **binary feature** for **each category of each categorical column** in the data. Although to be more specific, we limited it to columns whose type is `category` and excluded columns whose type is `object`. This is because one-hot encoding can quickly blow up the number of columns in the data if we are not careful and include categorical columns with lots of categories (also called **high-cardinality** categorical columns). In such cases, we would one-hot encode only the top $n$ categories (what $n$ should be and what \"top\" should mean depends on the context). Another \n",
    "\n",
    "What is the point of doing this? The reason we do this is that most machine learning algorithms do not work **directly** with categorical data, so we need to encode the categorical data which turns it into numeric data. One-hot encoding is just one type of encoding, but it is the most common one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last note about the `sklearn` pre-processing transformations we learned about in this notebook: If you look at examples online, you may notice that instead of calling `fit` and `transform` separately, you can call `fit_transform` which combines the two steps into one. This may seem reasonable and saves you one extra line of code, but we discourage it. The following exercise will illustrate why, but the main reason will become clear when we talk about machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (5 minutes)\n",
    "\n",
    "We want to one-hot-encode the `education` and `month` columns in bank. \n",
    "\n",
    "- First create an instance of the one-hot encoder and invoke fit and transform with fit_transform using the `education` and `month` columns in bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 017\n",
    "cols = ['education','month']\n",
    "binary_maker = OneHotEncoder(sparse_output=False)\n",
    "binary_data = binary_maker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new \"hybrid\" column names for the new one-hot-encoded values.  The column names are a hybrid of the original column name and the column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe out of the one-hot-encoded values and the new column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the new dataframe of the new one-hot-encoded columns to the original bank dataframe and then drop the original `education` and `month` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here 020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final note about one-hot encoding: there are more sophisticated way that we can perform one-hot encoding by using hash functions. We leave the reader to read about **feature hashing** (if you know what **hash functions** are, then feature hashing is very easy to grasp). This method has the advantage that we do not need to know what the categories are ahead of time. Not only does this free us from keeping track of the categories, but it also means that we don't have to load the pre-defined list of categories with the model **at score time** when the model is deployed: We just use the hash function to compute it on the fly. This can make scoring much more efficient when the size of features is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "The assignment is in **Lesson_05_b_assignment.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
