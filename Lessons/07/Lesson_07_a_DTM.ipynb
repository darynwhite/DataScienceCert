{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e2dd94",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The TF-IDF matrix can be the input data for machine learning algorithms\n",
    "- The source of the TF-IDF is the Document-Term Matrix (DTM)\n",
    "- The Term Frequencies are derived from the DTM \n",
    "- The Inverse Document Frequency vector (IDF) is derived from the DTM\n",
    "- The term frequency inverse document frequency (TF-IDF) is created by multiplying the TF with the IDF\n",
    "\n",
    "The **TF-IDF** matrix is used as an input for machine learning to:\n",
    "  - Characterize writing styles\n",
    "  - Find plagiarism\n",
    "  - Curate legal documents\n",
    "  - Identify fake news\n",
    "  - Determine Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95392d34",
   "metadata": {},
   "source": [
    "## Document Matrix Term (DTM)\n",
    "\n",
    "Further information on the DTM can be found here, **[Document-term matrix (DTM)](https://en.wikipedia.org/wiki/Document-term_matrix)**<br/>   \n",
    "The transpose of the DTM is the Term Document Matrix (TDM)\n",
    "\n",
    "When we have a corpus with some basic text pre-processing applied, we can create a **term document matrix (DTM)**. The DTM is a representation of **[Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)** model. The DTM has the following properties:\n",
    "\n",
    "- Number of occurences for a given term or word in a document are the DTM values\n",
    "- The DTM is a sparse matrix, as most documents do not include most terms. Sparse matrix coding should be used for efficiency. \n",
    "- The DTM is used to create the **Term Frequency** matrix\n",
    "- The DTM is used to create the **Inverse Document Frequency** vector\n",
    "\n",
    "Let's look at an example of a DTM. The figure in the lecture slides shows a corpus of text documents on the left. This corpus is transformed into the document term matrix shown on the right. Notice that the matrix is sparse as any given document may not contain a term.  Some documents may contain a term multiple times. \n",
    "\n",
    "## Create an example DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d164fd",
   "metadata": {},
   "source": [
    "### Corpus\n",
    "First we need a corpus.  A corpus is a collection of **documents**.  A document is any independent piece of text, like a Shakespeare play or a tweet.  The following list is a corpus with 5 documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus with 5 documents\n",
    "corpus = [\n",
    "    'i think machine learning is much fun',\n",
    "    'i think learning is fun',\n",
    "    'i think machines can learn to learn',\n",
    "    'i think coding is fun fun fun',\n",
    "    'i think i can i can'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad54afd",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "Next we need to establish the vocabulary because each unique term will be a row in the TDM.  The vocabulary is a set of unique terms.  A term is like a word in a text.  Terms and words are often referred to as tokens.  Splitting a text into its words or terms is called tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71682166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary\n",
    "vocabulary = set()\n",
    "for text in corpus:\n",
    "    terms = text.split(' ')\n",
    "    vocabulary.update(set(terms))\n",
    "    \n",
    "# cast set as list, which establishes order and allows indexing\n",
    "vocabulary = list(vocabulary)\n",
    "print('Vocabulary Size: {} distinct terms:'.format(len(vocabulary)))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0cae2",
   "metadata": {},
   "source": [
    "### Filling in the DTM\n",
    "1. The matrix is initialized with zeros.  The matrix has one row for each document and one column for each term.\n",
    "2. Each document of the corpus is tokenized and each token is compared to the vocabulary.  A match of the token with the vocabulary term increases the value for the cell specified by term and document\n",
    "\n",
    "#### Bag of Words\n",
    "The DTM is considered a kind of \"Bag of Words\" (BOW) model of the corpus.  The more traditional BOW model would be the Boolean representation of the DTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0388368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty term-document matrix (TDM):\n",
    "DTM = np.zeros((len(corpus), len(vocabulary)), dtype=np.intc)\n",
    "\n",
    "# Document term matrix example\n",
    "# fill the Document term matrix where\n",
    "# each row is for a different document and\n",
    "# each column is for a different term\n",
    "for doc_index, text in enumerate(corpus):\n",
    "    tokens = text.split(' ')\n",
    "    term_index = [vocabulary.index(token) for token in tokens if token in vocabulary]\n",
    "    for term_ix_col in term_index:\n",
    "        DTM[doc_index, term_ix_col] = DTM[doc_index, term_ix_col] + 1\n",
    "\n",
    "print(\"Example Document Term Matrix (DTM)\")\n",
    "DTM_df = pd.DataFrame(data=DTM, index=corpus, columns=vocabulary)\n",
    "display(DTM_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a4e17",
   "metadata": {},
   "source": [
    "### Term Frequencies\n",
    "Term Frequencies (TF) is DTM normalized by document length and complexity.  The reasoning for the normalization is:  If a document is very long or complex, then it is more likely to have some unusual terms.  In this case unusual terms would not be a marker for a machine learning outcome but rather a marker for document length.  To correct for this issue, we divide the counts in the raw by the length of the document.  The term frequencies for every document will sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique terms within each document\n",
    "NumberOfTerms = DTM_df.sum(axis = 1)\n",
    "print(\" Number of unique terms:\")\n",
    "display(NumberOfTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequencies (TF) is the normalized DTM\n",
    "# Divide every value by the number of terms in that document\n",
    "TF = DTM_df.mul(1/NumberOfTerms, axis=0)\n",
    "display(TF.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb45fc",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### TDM vs DTM\n",
    "DTM is the transpose of TDM.  Both have the same information.  DTM is organized in the sklean way for predicting a document attribute, like sentiment or topic.   \n",
    "\n",
    "#### We should have cleaned the texts first.  Some text cleaning methods:  \n",
    "- Reduce number of terms (dimensions) by stemming or lemmatization:\n",
    "    - We could stem \"learning\" and \"learn\" to make a single term called \"learn\"\n",
    "    - We could stem \"machine\" and \"machines\" to make a single term called \"machine\" \n",
    "- Remove stop words.  Some words (terms) are very common.  Such words are called stop words and have little meaning.  Examples are: \"i\", \"to\", and \"is\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd0078",
   "metadata": {},
   "source": [
    "## Create IDF\n",
    "\n",
    "If a term occurs in too many documents, then that term will not help in machine learning.  Somewhat rare terms are more interesting.\n",
    "To that end, we determine the inverse document frequency (IDF) for every term.  For each term:\n",
    "- Determine number of documents with the term\n",
    "- Divide total number of documents by the number of documents with the term\n",
    "- Take log of the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents in which the term appears\n",
    "NumberOfDocsWithTerm = (DTM_df > 0).sum(axis = 0)\n",
    "\n",
    "# Inverse Document Frequency (IDF) for each term\n",
    "TotalNumberOfDocs = DTM_df.shape[0]\n",
    "IDF = np.log(TotalNumberOfDocs/NumberOfDocsWithTerm)\n",
    "\n",
    "# Present results\n",
    "pd.DataFrame(data=(NumberOfDocsWithTerm,IDF), index=['#Docs', 'IDF']).T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf6293",
   "metadata": {},
   "source": [
    "The above formula works well in our dataset.  It is:\n",
    "$$IDF = Log\\Biggl(\\dfrac{T}{N}\\Biggr)$$\n",
    "where:\n",
    "- T is Total Number Of Docs in corpus\n",
    "- N is vector of Number Of Docs that contain a Term  \n",
    "<br/><br/>\n",
    "\n",
    "For a robust deployment we need to prepare for cases where the NumberOfDocsWithTerm (N) is zero:\n",
    "$$IDF = Log\\Biggl(\\dfrac{T + 1}{N + 1}\\Biggr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f41bc",
   "metadata": {},
   "source": [
    "## Create a TF-IDF matrix\n",
    "We multiply the IDF vector into the TDM to create a TF-IDF matrix.  The IDF vector is multiplied element-by-element with each document of the TDM.\n",
    "\n",
    "$$\\text{TF-IDF} = TF âŠ™ IDF$$\n",
    "where:\n",
    "- TF is the term-frequency matrix\n",
    "- IDF is the inverse document frequency vector\n",
    "- TF-IDF is the Term frequency - Inverse Document Frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = TF.mul(IDF, axis=1)\n",
    "display(TF_IDF.round(decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a759c",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "- The TF-IDF matrix contains no nulls (nan).  How can we see that the TF-IDF is a sparse matrix?\n",
    "- Assume that each document in the training and test data sets are additionally labeled with a sentiment.  How would we combine the sentiment labels with the TF-IDF matrix?\n",
    "- How could the TF-IDF matrix be used in sentiment analysis?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
