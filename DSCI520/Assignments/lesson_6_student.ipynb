{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to hypothesis testing\n",
    "\n",
    "Let's say you have a large bag with red, green, and blue balls, and you stick your hand in the bag and draw a dozen balls out. All the balls in the large bag is what we call the **population** and the dozen balls we drew are a **sample**. **Probability** is concerned with what our draw looks like based on how many balls of each color we have in the large bag. In other words, probability asks, given what we have in the population, what should the sample look like. **Statistics** asks the opposite: Given what color balls we drew, what can we say about the composition of balls in the population. In other words, given what we see in the sample, what can we conclude about the population? Drawing conclusions about the population based on the sample is called **statistical inference**.\n",
    "\n",
    "At the conclusion of this lesson you should be able apply basic classical hypothesis tests to some common situations. In addition to the usual libraries, we will also be using `statsmodels` and `scipy` for many of our examples. Let's begin by loading all the libraries. In this notebook we cover the following examples of tests of hypothesis:\n",
    "\n",
    "- Two sample tests for continuous variables\n",
    "- Two sample tests for categorical or count data\n",
    "- Test for distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(rc = {'figure.figsize': (8, 12)})\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import scipy.stats as ss\n",
    "import numpy.random as nr\n",
    "import statsmodels\n",
    "import statsmodels.stats as st\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.power as sp\n",
    "import statsmodels.stats.weightstats as ws\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset derived from Vladimir Belyaev's [steamdb database](https://github.com/leinstay/steamdb) of all the games offered on the Steam platform to look at various examples of statistical tests. The curated subset of games was restricted to those with complete information and high popularity. It has the following columns:\n",
    "- name: video game name \n",
    "- store_uscore - store.steampowered.com user's score\n",
    "- igdb_uscore  - igdb.com critic's score\n",
    "- meta_score - metacritic.com critic's score\n",
    "- gfq_rating - gamefaqs.gamespot.com user ratings\n",
    "- windows_only: Available for Windows OS exclusively, or also on Mac / Linux\n",
    "    - 0: Available on other OS\n",
    "    - 1: Windows OS exclusive\n",
    "- price: December 2022 price on steampowered.com\n",
    "    - 1: < $19.99\n",
    "    - 2: $19.99 - $24.99\n",
    "    - 3: > $24.99\n",
    "- difficulty: gamefaqs.gamespot.com difficulty rating\n",
    "    - 1: Simple / Easy\n",
    "    - 2: Just Right\n",
    "    - 3: Tough / Unforgiving\n",
    "- time: howlongtobeat.com main story completion time in hours\n",
    "    - 1: <= 6 hours\n",
    "    - 2: 7 - 10 hours\n",
    "    - 3: 11 - 20 hours\n",
    "    - 4: 21+ hours\n",
    "- popularity: igdb.com popularity rating. A minimum score of 5 was used to eliminate unpopular games.\n",
    "    - 1: popularity 5-25\n",
    "    - 2: popularity > 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_json('../../data/steamdb_popular.json')\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student's t-test for comparing averages\n",
    "\n",
    "The difference in means between two normal distributions with unknown variance follows a t-distribution. The t-test is one of the oldest and most widely used classical hypothesis tests. It can be used to\n",
    "\n",
    "- test whether a population mean has a specified value (**one-sample** t-test).\n",
    "- test the difference between two means  of two samples (with equal, or unequal variances) (**two-sample** t-test)\n",
    "- test a paired-response difference from zero, e.g., a before/after drug treatment on patients (**paired** t-test)\n",
    "- test whether the slope of a line is not zero (covered later in class)\n",
    "- test the importance of variables (covered later in class)\n",
    "\n",
    "As a first example, compare the average metacritic score for windows only releases vs multi-platform games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = scores.loc[scores['windows_only'] == 1, 'meta_score']\n",
    "multi_platform = scores.loc[scores['windows_only'] == 0, 'meta_score']\n",
    "\n",
    "bins = [x for x in range(20,101,5)]\n",
    "\n",
    "sns.displot(data = scores,\n",
    "            x = 'meta_score', \n",
    "            hue = scores['windows_only'], \n",
    "            bins = bins, \n",
    "            palette = {0:'red', 1:'blue', 'desat':0.5},\n",
    "            kde = True,\n",
    "            aspect = 1.75)\n",
    "\n",
    "plt.title('Distribution of metacritic scores by OS platform availability')\n",
    "plt.axvline(windows.mean(), 0, color = 'blue', linestyle = '--')\n",
    "plt.axvline(multi_platform.mean(), 0, color = 'red', linestyle = '--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the values of the distributions of these populations overlap quite a bit and the means are close. The question is, are these differences significant?  \n",
    "\n",
    "You can now compute the two-sample t-test to determine if the difference of means is significant. A number of summary statistics are computed and printed for the test. The two-sided t-test is used to determine if we can reject the **null hypothesis that the difference of means is not significant**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to implement the t-test manually based on what is described [in this Wikipedia article](https://en.wikipedia.org/wiki/Student%27s_t-test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_win, n_multi = windows.size, multi_platform.size\n",
    "mean_win, mean_multi = windows.mean(), multi_platform.mean()\n",
    "var_win, var_multi = windows.var(), multi_platform.var()\n",
    "var_P = var_win / n_win + var_multi / n_multi\n",
    "\n",
    "t_stat = (mean_win - mean_multi) / var_P**0.5\n",
    "deg_f = var_P ** 2 / ((var_win / n_win)**2 / (n_win - 1) + (var_multi / n_multi)**2 / (n_multi - 1))\n",
    "p_val = ss.t.cdf(t_stat, df = deg_f) * 2\n",
    "\n",
    "print(\"statistic = {} and p-value = {}\".format(t_stat, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that we obtain a p-value of around 0.0008. Since this p-value is very low, we reject the null hypothesis and conclude that the two population means are significantly different.\n",
    "\n",
    "Instead of doing all the calculation manually, which can be a tedious, we can also use the `statsmodels` package to take care of the computation for us and present us with the results (including additional calculations such as confidence intervals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(a, b, alpha = 0.05, alternative = 'two-sided', type = 'independent'):\n",
    "\n",
    "    diff = a.mean() - b.mean()\n",
    "    if type == 'independent':\n",
    "        res = ss.ttest_ind(a, b, equal_var = False)\n",
    "    elif type == 'paired':\n",
    "        assert(len(a) == len(b))\n",
    "        res = ss.ttest_rel(a, b)\n",
    "      \n",
    "    means = ws.CompareMeans(ws.DescrStatsW(a), ws.DescrStatsW(b))\n",
    "    confint = means.tconfint_diff(alpha = alpha, alternative = alternative, usevar = 'unequal') \n",
    "    degfree = means.dof_satt()\n",
    "\n",
    "    index = ['deg_of_freedom', 'observed_difference', 't-statistic', 'p_value', 'low_95_CI', 'high_95_CI']\n",
    "    return pd.Series([degfree, diff, res[0], res[1], confint[0], confint[1]], index = index)   \n",
    "   \n",
    "\n",
    "t_test(windows, multi_platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either way we get the same t-statistic and p-value. Examine these statistics noticing the following:\n",
    "\n",
    "1. Difference in means is not enormous, 2.7 points on a 100 point scale, and the populations overlap quite a bit, but the t-statistic is large in absolute value and the p-value is small. Therefore, there is a small chance that the difference in means is from random variation alone. \n",
    "3. The 95% confidence interval does not straddle 0.  \n",
    "\n",
    "Based on these statistics we can reject the null hypothesis and conclude that the two samples did not come from populations with the same mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical power analysis\n",
    "\n",
    "The **power of a test** is formally defined $\\text{power} = P(\\text{reject } H_0| \\text{ when } H_0 \\text{ is false})$. In plain language, the power of a test is the probability of finding a true effect to be a significant effect. A test with insufficient power will not detect a significant effect even when it should. Clearly, we want the most powerful test we can find for the situation. Computing power can be a bit complex, and analytical solutions can be difficult or impossible. Often, a simulation is used to compute power.\n",
    "\n",
    "Let's look at the example of computation power for the two sample t-tests for the difference of means. The power of this test depends on the several parameters:\n",
    "\n",
    "- the number of samples\n",
    "- the difference in the sample means standardized by their overall variability (also known as the **effect size**)\n",
    "- the type of test\n",
    "\n",
    "When running a power test, you can ask several questions that will assist you in designing an experiment. Usually, you will determine how big a sample you need to have a good chance of rejecting the null hypothesis. You can also determine how big an effect needs to be given a fixed sample size (all the samples you have or can afford) to have a good chance of rejecting the null hypothesis. \n",
    "\n",
    "The `statsmodels` package provides power calculations for a limited set of hypothesis tests. We can use these capabilities to examine the power. We can use it to find the right sample size for an effect size.\n",
    "\n",
    "\n",
    "The `statsmodels` package provides power calculations for a limited set of hypothesis tests. We can use these capabilities to examine the power. We can use it to find the right sample size for a effect size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = np.arange(start = 2, stop = 12, step = 1)\n",
    "power = [sp.tt_ind_solve_power(effect_size = abs(t_stat), nobs1 = n, alpha = 0.05, \n",
    "                               power = None, ratio = 1.0, alternative = 'two-sided') \n",
    "          for n in sample_size]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (12,5))\n",
    "sns.lineplot(x = sample_size, y = power, ax=  ax)\n",
    "plt.title('power vs. sample size')\n",
    "plt.xlabel('sample size')\n",
    "plt.ylabel('power');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, less than $80\\%$ power for a test is not good. We can also use it to find the right effect size for a given sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size = np.arange(start = 0.0, stop = 10.0, step = .2)\n",
    "power = [sp.tt_ind_solve_power(effect_size = x, nobs1 = 25, alpha = 0.05, \n",
    "                               power = None, ratio = 1.0, alternative = 'two-sided') \n",
    "          for x in effect_size]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize =(10,5))\n",
    "sns.lineplot(x = effect_size, y = power, ax = ax)\n",
    "plt.axvline(abs(t_stat), 0, color = 'blue') # marks the observed effect\n",
    "plt.title('power vs. effect size')\n",
    "plt.xlabel('effect size')\n",
    "plt.ylabel('power');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results and notice how the power of the t-test rapidly increases as the difference in means increases. At a relatively small difference in means the power of the test is approaching 1.0, the maximum possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "- Find out if there is a significant difference in metacritic scores between platform distributions (`windows_only`) if we limit the data to the most popular games only: `popularity` == 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_pop = scores.loc[((scores['windows_only'] == 1) & (scores['popularity'] == 2)), 'meta_score']\n",
    "multi_platform_pop = scores.loc[((scores['windows_only'] == 0) & (scores['popularity'] == 2)), 'meta_score']\n",
    "t_test(windows_pop,multi_platform_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find out if there is a significant difference between user scores on IGDB (`igdb_uscore`) and on Steam, (`store_uscore`) for the most popular games (`popularity` == 2). HINT: For this you need to perform a paired t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $P-value < \\alpha$, we reject the null hypothesis and conclude that IGDB and Steam scores for popular games are significantly different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that we can fail to reject the null hypothesis if the test lacks power. Let's investigate that.\n",
    "\n",
    "- Plot 100 values for effect size ranging from $0.0$ to $10.0$ and measure the effect size required for the above test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for categorical data\n",
    "\n",
    "The t-test is used on continuous data. What if we have categorical data?\n",
    "\n",
    "### Chi-squared tests\n",
    "\n",
    "We use Chi-squared (also written as $\\chi^2)$ tests to compare counts in different categories, where the categories are mutually exclusive.\n",
    "\n",
    "- A Chi-squared **goodness of fit** test (also called **test of proportions** or **Pearson's Chi-squared test**) is used to test if the sample is representative of the population. For example, if we survey people randomly in Washington State, we can test if the number of respondents in Seattle is significantly different from the expected probability based on Seattle's percentage of the state population.\n",
    "- The Chi-squared **test of independence** (also called **test of association**) tests whether two categorical variables are related to each other. It does so by comparing observed counts to counts we should expect if the two variables are independent.\n",
    "\n",
    "The name Chi-squared comes from a corresponding probability distribution. It's density depends on a parameter we call **degrees of freedom**, which is greater the bigger the sample size.\n",
    "\n",
    "![](https://library.startlearninglabs.uw.edu/DATASCI410/img/Chi-square.png)\n",
    "\n",
    "As with any probability density function, confidence intervals and p-values can be computed. Notice that the $\\chi$-squared distribution becomes flatter and with greater dispersion as the degrees of freedom increase. In practice, this means that you will need large samples to get a meaningful result if you have too many choices in your test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-squared test of proportion example\n",
    "\n",
    "Here's an example of the Chi-squared goodness of fit: The traditional way to apply a chi-squared test is to first create a chi-squared table. In this example we are looking at the results of an A-B test with three possible outcomes. This type of test might be applied to determine if a new web site drives more customer purchases.\n",
    "\n",
    "The code in the cell below builds a simple chi-squared table. The data we have are\n",
    "\n",
    "- actual occurrence of events\n",
    "- expected probability of these events (based on the distribution of the null hypothesis)\n",
    "\n",
    "From the above data we then calculate the following\n",
    "\n",
    "- the expected occurrence of events given the expected probabilities\n",
    "- the difference between the occurrence and the expected number of events\n",
    "- the square of the difference\n",
    "- the squared difference normalized by the expected number of occurrences\n",
    "\n",
    "The sum of the figures in the last column is the Chi-squared statistic. In other words \n",
    "\n",
    "$$\\chi^2_{\\text{statistic}} = \\sum_{i = 1}^k \\frac{(o_i - e_i)^2}{e_i}$$ \n",
    "\n",
    "where we have $k$ groups, $o_i$ is the observed count for group $i$ and $e_i$ is the expected count for group $i$ based on the hypothesized proportions. Under $H_0$ we assume $\\chi^2_{\\text{statistic}} \\sim \\chi^2(k-1)$ where $k-1$ are the degrees of freedom.\n",
    "\n",
    "In the complete steamdb dataset, proportion of games in the price brackets for `price` (1: < $19.99, 2: $19.99 - $24.99, 3: > $24.99)\n",
    "is 88 percent for low, 8 percent for middle and 4 percent for high. Here are the counts we observe in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_prices = [0.88, 0.08, 0.04]\n",
    "price_counts = scores['price'].value_counts(sort = False)\n",
    "price_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually compute the $\\chi^2$ statistic as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = pd.DataFrame({'price': ['low', 'middle', 'high'],\n",
    "                         'observed_count': price_counts,\n",
    "                         'hypothesized_proportion': all_prices})\n",
    "\n",
    "total_count = price_data['observed_count'].sum()\n",
    "price_data['expected_count'] = total_count * price_data['hypothesized_proportion']\n",
    "\n",
    "price_data['diff_squared'] = (price_data['observed_count'] - price_data['expected_count'])**2\n",
    "price_data['diff_normalized'] = price_data['diff_squared'] / price_data['expected_count']\n",
    "\n",
    "chi_squared_statistic = price_data['diff_normalized'].sum()\n",
    "print(\"chi_squared_statistic = {}\".format(chi_squared_statistic))\n",
    "price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $3-1 = 2$ degrees of freedom for the $\\chi^2$ distribution. We can get a p-value for this statistic now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - ss.chi2.cdf(chi_squared_statistic, df = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This p-value is rather small, so we reject the null hypothesis. In other words, assuming the hypothesized proportions for each group, there is little chance that the differences between the observed and expected occurrences are the result of random variation alone.\n",
    "\n",
    "In the foregoing example we computed the chi-squared statistic and p-value directly. In general, this is a somewhat cumbersome approach. Instead, we can use the `chisquare` function from the `scipy.stats` package as shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sq_stat, p_value = ss.chisquare(price_data['observed_count'], price_data['expected_count'])\n",
    "print('Chi-squared statistic = {} and p-value = {}.'.format(str(chi_sq_stat), str(p_value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we should check the power of our test. First we compute our observed effect size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_size = statsmodels.stats.gof.chisquare_effectsize(price_counts / price_counts.sum(), all_prices)\n",
    "eff_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see where we are by plotting effect size against power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size = np.arange(start = 0.1, stop = 2.0, step = 0.1)\n",
    "power = sp.GofChisquarePower().solve_power(effect_size = effect_size, nobs = price_counts.sum(), \n",
    "                                           n_bins = 3, alpha = 0.05)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize =(10,5))\n",
    "sns.lineplot(x = effect_size, y = power, ax = ax)\n",
    "plt.axvline(abs(eff_size), 0, color = 'blue') # marks the observed effect\n",
    "plt.title('power vs. effect size')\n",
    "plt.xlabel('effect size')\n",
    "plt.ylabel('power');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, this test is quite powerful, and we could have detected much smaller differences between the observed and expected counts.\n",
    "\n",
    "Let's perform a Chi-squared test of independence to see if two categorical variables are related or not. Let's see if the difficulty of a game is related to the time it takes to complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(scores['time'], scores['difficulty'])\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sq_stat, p_value, dof, ct_expected = ss.chi2_contingency(ct)\n",
    "print('Chi-squared statistic = {} and p-value = {}.'.format(str(chi_sq_stat), str(p_value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the effect size by flattening the observed and expected counts tables and turning counts into percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ct.values.ravel() / ct.values.sum()\n",
    "exp = ct_expected.ravel() / ct_expected.sum()\n",
    "\n",
    "eff_size = statsmodels.stats.gof.chisquare_effectsize(obs, exp)\n",
    "eff_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we return to the chart of power vs effect size we would see that this test has just enough power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing distributions\n",
    "\n",
    "Tests of hypotheses often make distributional assumptions. This raises the question of how can we test if our data fits some distribution. To answer this question we will now look at both graphical and formal tests. Since in most cases, distribution assumptions are approximate, simple graphical methods are often sufficient.\n",
    "\n",
    "### The Q-Q plot\n",
    "\n",
    "The quantile-quantile (Q-Q) plot provides a handy visual means to inspect the similarity of distributions of a data set. The general idea is to plot the quantiles of the sample on the vertical axis and the quantiles of the theoretical distribution on the horizontal axis. If the points of the plot fall on an approximately straight line, you can conclude that the sample distribution is close to the theoretical. \n",
    "\n",
    "The normal Q-Q plot plots the quantiles of a standard normal distribution on the horizontal axis and the quantiles of the data sample on the vertical axis. If the sample is normal the data points will fall in a straight line. \n",
    "\n",
    "Run the code in the cell below to create Q-Q plots of the two samples, and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols = 2, figsize = (12,5))\n",
    "sm.qqplot(scores['igdb_uscore'], line = 's', ax = axs[0])\n",
    "sm.qqplot(scores['meta_score'], line = 's', ax = axs[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of points on both Q-Q plots is close to straight, but with significant deviations on the tails indicating less dispersion than a standard normal distribution.\n",
    "\n",
    "We can also visualize the **cumulative density functions** (CDFs) of the two samples. To make things easier, here's a function that plots cumulative distributions for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cums(dist1, dist2, labels = ['sample 1', 'sample 2']):\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    p = np.arange(len(dist1)) / (len(dist1) - 1) # calculate the proportional values of samples\n",
    "\n",
    "    data_sorted = np.sort(dist1) # sort the first data sample\n",
    "    sns.lineplot(x = data_sorted, y = p)\n",
    "    \n",
    "    data_sorted = np.sort(dist2) # sort the second data sample:\n",
    "    sns.lineplot(x = data_sorted, y = p)\n",
    "\n",
    "    plt.title('empirical CDFs of two datasets')\n",
    "    plt.xlabel('range of data')\n",
    "    plt.ylabel('percentile')\n",
    "    plt.legend(labels = labels)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cums(scores['meta_score'], scores['igdb_uscore'], labels = ['meta_score', 'igdb_uscore'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since IGDB and Metecritic scores are on the same scale we didn't need to rescale the data, but in general we need to rescale the data prior to comparing the distributions. The GameFAQ ratings are on a 5 point scale, so we can directly compare with the 100 point ratings by normalizing the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_norm = (scores['store_uscore'] - scores['store_uscore'].mean()) / scores['store_uscore'].std()\n",
    "gfq_norm = (scores['gfq_rating'] - scores['gfq_rating'].mean()) / scores['gfq_rating'].std()\n",
    "\n",
    "plot_cums(store_norm, gfq_norm, labels = ['Store User Score', 'GameFAQ Rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, as you might expect, the two CDFs are quite similar, indicating that the two samples are drawn from the same distribution. But are the deviations significant? We can answer that more formally using the **Kolmogorov-Smirnov test**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kolmogorov-Smirnov test\n",
    "\n",
    "We can test whether a sample follows a theoretical distribution or if two distributions are significantly different by using the Kolmogorov-Smirnov test. The K-S statistic is just the maximum vertical distance between the CDF of the sample and the CDF of the theoretical distribution, or the CDFs of two independent samples. Since it is based on a simple deviation, the K-S test can test departure from any hypothetical distribution, not just normal.\n",
    "\n",
    "The **K-S statistic is the maximum vertical difference** between the two cumulative density functions. Based on this distance and the number of samples, the p-value for the K-S test is computed. It is important that the **samples must be standardized** before applying the K-S test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare user score on the Steam store with a standard normal\n",
    "ss.kstest(store_norm, 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare user score on the Steam store with GameFAQs user's scores\n",
    "ss.kstest(store_norm, gfq_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is small when compared with a standard normal, therefore we reject the null hypothesis that the data is normally distributed (with $\\mu = 0$ and $\\sigma = 1$). In other words, assuming that the data is normally distributed, a sample like ours is quite unusual.\n",
    "\n",
    "The p-value when comparing the user ratings on two different platforms shown a much more similar distribution even though the raw scales were different. \n",
    "\n",
    "The K-S test is rather general, as it can be applied to test any distribution. However, this means that the **power** of this test is limited. The power of a test is the probability of rejecting a null hypothesis when the alternative is true. \n",
    "\n",
    "As is always the case with classical statistics, a more powerful test can be created by adopting more restrictive assumptions. For example, the Shapiro-Wilk test has greater power, but is specifically for normal distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing multiple groups with ANOVA\n",
    "\n",
    "So far, we have only looked at tests for comparing two samples. What if we have multiple groups and we wanted to compare their means? Why can’t we just do multiple two-sample t-tests for all pairs? Because doing so results in increased probability of accepting a false hypothesis; e.g., if we had 7 groups, there would be $7 \\choose 2$ $=21$ pairs to test.  Recall that the $\\alpha$ cutoff is the probability of committing a type-I error, so if $\\alpha = 0.05$ for any one test then we have a 95% probability of rejecting $H_0$ correctly. But this probability drops as we perform more tests: for 21 tests it is $0.95^{21} \\approx 0.34$!\n",
    "\n",
    "This problem is known as the problem of **multiple comparisons** or **base rate fallacy**. With large numbers of groups, there is a high probability of getting a false positive (type I error). Several adjustments to the multiple comparisons problem have been proposed, such as the **Bonferroni correction**: instead of $\\alpha$ we use $\\alpha_b = \\frac{\\alpha}{k}$ (where $k$ is the number of groups) as our p-value threshold. This tends to be a conservative approach that won't work well if $k$ is very high. There are other approaches that we won't get into here.\n",
    "\n",
    "Instead of trying to find out all possible significant effects like we do above, we can first perform a test to see if there are *any* significant effects, without necessarily knowing which:\n",
    "\n",
    "- Null hypothesis: All groups are samples from the same population.\n",
    "- Alternative hypothesis: At least one group has a statistically different mean.\n",
    "\n",
    "This type of analysis is called “ANalysis Of VAriance”, or ANOVA for short. ANOVA is one of a large family of models used for **experimental design**.\n",
    "\n",
    "Let's have a look at how we would perform the comparisons between the multiple groups of data. First, make data independence and normality assumptions about the groups. Let $k$ be the number of treatment groups, $n$ be the number of data points and SS be the sum of squares. We can split total sum of squares **SSTotal** into the sum of squared errors *between* treatments **SST** and the sum of squared errors *within* treatments **SSE**: SSTotal = SST + SSE. We can divide each by its corresponding degrees of freedom to get the mean squared error between treatments **MST** and the mean square error within treatments **MSE**. Finally we can compute the F-statistic and derive the p-value by referring to the [F distribution](https://en.wikipedia.org/wiki/F-distribution). \n",
    "\n",
    "We leave out the details of the exact calculations, and instead we summarize the results in an ANOVA table as follows:\n",
    "\n",
    "|Type|Sum of Squares|degrees of freedom|Mean Square E|F|Significance|\n",
    "|---|:---:|:---:|:---:|:---:|:---:|\n",
    "|between groups|SST|$k-1$|SST / $k-1$|F-statistic| p-value|\n",
    "|within groups|SSE|$n-k$|SSE / $n-k$|||\n",
    "|total|SSTotal|$n-1$||||\n",
    "\n",
    "Let's now see an example. Let's say we wanted to test if on average the Metacritic scores are different for games with degrees of difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows= 1, ncols=1, figsize=(10,5))\n",
    "sns.boxplot(x = 'difficulty', y = 'meta_score', data = scores, ax= ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows variation between the distributions of the groups, but are any of these differences significant? To answer that we use an ANOVA test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('meta_score ~ C(difficulty)', data = scores).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F statistic is fairly large and the p-value is small. So we reject the null hypothesis and conclude that there are significant differences between the different groups.\n",
    "\n",
    "Note that ANOVA isn't just a test, but also a statistical model. This means that once we fit it to the data, we can then use it to obtain predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict({'difficulty': [1, 2, 3]}) # we get a unique prediction for each value of difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the distribution (histogram) of the actual scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(scores['meta_score'], \n",
    "            kde = False, \n",
    "            color = 'blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the predicted scores are less varied than the actual scores. In fact, the model predicts only three distinct scores. This should not come as a surprise, since we trained a model to predict the Metacritic score using **only** the `difficulty` rating from gamespot.com (ignoring all the other variables). So this model is returning a unique score for each difficulty group. We can make this model richer by adding more variables to it, but that's the topic of another lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tukey's multiple comparison test\n",
    "\n",
    "From the above ANOVA results we know that there is some difference in the means of these groups. However, ANOVA does not tell us which pairs of groups are significantly different from each other. From the box plot, we could make an educated guess, but one formal way of comparing groups is using **Tukey's multiple comparison** test. The test returns the following for each pairing of groups:\n",
    "\n",
    "- the difference of the means\n",
    "- the confidence interval of the difference in the means\n",
    "- the p-value from the distribution of the differences\n",
    "\n",
    "These results are laid out in a table or can be plotted graphically. Only differences in means with a confidence interval not overlapping zero are considered significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tukey_HSD = pairwise_tukeyhsd(scores['meta_score'], scores['difficulty'])\n",
    "print(Tukey_HSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the table above. If the difference in means between the variables is significant, the confidence interval will not include 0. Which pairs have a significant difference at the 95% confidence level? You can see the results of this test in the rightmost column of the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In this assignment, we want to reinforce the concepts we covered in the lecture. Let's first load the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(rc = {'figure.figsize': (10, 8)})\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import scipy.stats as ss\n",
    "import numpy.random as nr\n",
    "import statsmodels\n",
    "import statsmodels.stats as st\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.power as sp\n",
    "import statsmodels.stats.weightstats as ws\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the automobile mileage data for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_auto_data(file = \"../data/canadian_cars_2022.csv\"):\n",
    "    'Function to load the auto data set from a .csv file' \n",
    "\n",
    "    ## Read the .csv file with the pandas read_csv method\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    ## Split the number of gears from the type of transmission, decode fuel\n",
    "    df['gears'] = df['transmission'].str.extract(r'([0-9]+)').astype('Int64')\n",
    "    df['gears'] = df['gears'].fillna(1) # \"gearless\" continuously_variable vehicles\n",
    "    df['fuel'].replace({'X': 'regular_gas', \n",
    "                             'Z': 'premium_gas', \n",
    "                             'D': 'diesel'}, inplace = True)\n",
    "    df['transmission'] = df['transmission'].str.extract(r'([A-Z]+)')\n",
    "    df['transmission'].replace({'A': 'automatic', \n",
    "                             'AM': 'automated_manual', \n",
    "                             'AS': 'automatic_select_shift', \n",
    "                             'AV': 'continuously_variable', \n",
    "                             'M': 'manual'}, inplace = True)\n",
    "    \n",
    "    ## Remove rows with missing values\n",
    "    df = df.dropna(axis = 0).reset_index(drop= True)\n",
    "    return df\n",
    "\n",
    "\n",
    "auto_df = read_auto_data()\n",
    "auto_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following tests on the data:\n",
    "\n",
    "1. Test whether `fuel_consumption_mpg` and log `fuel_consumption_mpg` (using `np.log10`) follow a normal distribution. Use both a **graphical** method and a **formal** test. For the rest of this exercise, choose between using mpg or log mpg based on which of the two best fits a normal distribution. <span style=\"color:red\" float:right>[5 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the QQ plots that the data appears to be right-tailed, with a few cars having very high mileage. Using a log transformation reduces the skew, but it also pushes the lowest mileages off the normal distribution curve as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Test if the fuel consumption is significantly different for the following populations of vehicles\n",
    "- \"Big 3\" North American brands ('buick', 'cadillac', 'chevrolet', 'chrysler', 'dodge', 'ford',  'gmc', 'jeep', 'lincoln') compared with brands that began in other countries\n",
    "- Vehicles with 1 gear vs many gears\n",
    "- Vehicle with greater than median height vs less than or equal the median height\n",
    "You are running separate tests for each variable. Use both graphical methods and the formal test. <span style=\"color:red\" float:right>[5 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply ANOVA and Tukey's HSD test to the miles per gallon to compare the fuel economy of autos for different vehicle classes. Restrict the analysis to just the `vehicle_class` categories having 10 or more cars in the data. Note that ANOVA and Tukey's HSD are **two separate tests**! <span style=\"color:red\" float:right>[5 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA tests whether there are any significant differences between any of the categories: $H_0: $ are categories have the same mean mpg, and $H_1: $ at least one category has a different mean mpg. If the p-value for ANOVA is significant, then we can perform a Tukey's HSD test to see which categories are significantly different from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Graphically explore the differences in mileage of the cars with different body styles. If any of these relationships are statistically significant (as suggested by Tukey's HSD), examine the sample size and decide if they should be considered practically significant. <span style=\"color:red\" float:right>[5 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "Note that to get full grade, for graphical tests you should include commentary on what your plot is showing. For formal tests should include the following:\n",
    "- begin by naming the test you are using\n",
    "- begin by clearly stating the null and alternative hypotheses\n",
    "- run the test and report the statistic and p-value\n",
    "- based on the p-value you should state the conclusion\n",
    "\n",
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "342.367px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "9cd5b1060645c161064a32399f3815b62036dca35fd6bfa9efb7b0f1d11b6ced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
