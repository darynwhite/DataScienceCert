{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 06 Assignment\n",
    "\n",
    "In this assignment, we want to read the `retail-churn.csv` dataset that we examined in a previous assignment and begin to pre-process it. The goal of the assignment is to become familiar with some common pre-processing and feature engineering steps by implementing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "col_names = ['user_id', 'gender', 'address', 'store_id', 'trans_id', 'timestamp', 'item_id', 'quantity', 'dollar']\n",
    "churn = pd.read_csv(\"../../data/retail-churn.csv\", sep = \",\", skiprows = 1, names = col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic EDA\n",
    "present the first few rows and use pandas' `describe` to get an overview of the data\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[0 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data frame will be called `churn_processed`, which stores the pre-processed columns as you run through each of the these steps. You will need to make sure your columns are properly named.\n",
    "\n",
    "1. Cast the `timestamp` column in churn into a column of type `datetime` and put the column into the `churn_processed` dataframe.  The new column should also be named `timestamp`.  Extract two new columns from `timestamp`: `dow` is the day of the week and `month` is the month of the year. Then drop the `timestamp` column from `churn_processed`.  Present the first few rows of `churn_processed`.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n",
    "# Create the new (empty) data frame, called churn_processed\n",
    "churn_processed = pd.DataFrame()\n",
    "# Cast timestamp to datetime\n",
    "churn_processed['timestamp'] = pd.to_datetime(churn.timestamp)\n",
    "# Create a dow column\n",
    "churn_processed['dow'] = churn_processed.timestamp.dt.day_of_week\n",
    "# Create a month column\n",
    "churn_processed['month'] = churn_processed.timestamp.dt.month\n",
    "# Drop Timestamp\n",
    "churn_processed.drop('timestamp',axis=1,inplace=True)\n",
    "# See what we have\n",
    "churn_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add `address` from `churn` to `churn_processed`. One-hot encode `address`, `dow` and `month`. Then drop columns `address`, `dow`, and `month` from `churn_processed`.  Finally, show some of the dataframe.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[2 point]</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n",
    "# add address column\n",
    "churn_processed['address'] = churn.address\n",
    "# One-hot-encode address, dow and month\n",
    "one_hot = OneHotEncoder(sparse_output=False)\n",
    "# Create Column Names\n",
    "one_hot.fit(churn_processed)\n",
    "# Add one-hot endcoded values to new columns\n",
    "churn_processed[one_hot.get_feature_names_out()] = one_hot.transform(churn_processed)\n",
    "# Drop address, dow and month\n",
    "churn_processed.drop(['dow','month','address'], axis=1, inplace=True)\n",
    "# Show the dataframe\n",
    "churn_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. So far we dropped `address`, `dow`, `month`, and `timestamp`.  Why would we want to drop all these columns?\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[1 point]</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why would we want to drop all these columns?**<br/>\n",
    "\n",
    "> We seem to be generating a dataframe that will search for any correlating or predictive features within the categorical data from the data source by encoding it into binary switches. Maintaining the source data columns is unnecessary and could cause bugs or problems as we continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Rescale `dollar` using min-max normalization. Use `pandas` and `numpy` to do it and call the rescaled column `dollar_std_minmax`.  Then see what the first few rows of the dataframe looks like.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[1 point]</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n",
    "# Min-max of dollar using numpy and pandas\n",
    "churn['dallar_std_minmax'] = (churn.dollar - churn.dollar.min())/(churn.dollar.max() - churn.dollar.min()) \n",
    "# See what the dataframe looks like\n",
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read about **robust normalization** [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html). The word **robust** in statistics generally refers to methods that behave reasonably even if the data is unusual.  For example, you can say that the median is a *robust* measure for the \"average\" of the data, while the mean is not.  In this respect a normalization is similar to an average. For a normalization **robust** might mean that the method is not affected by outliers. \n",
    "<br/><br/>\n",
    "5. \n",
    "  - Write briefly about what makes robust normalization different from Z-normalization.  \n",
    "  - Write briefly about what makes robust normalization more robust than Z-normalization.  \n",
    "  - Rescale `quantity` using robust normalization. Call the rescaled column `qty_std_robust` and add it to `churn_processed`.  \n",
    "  - Compare minimum, maximum, mean, standard deviation, and the median of the original churn['quantity'] with the robust-normalized churn_processed['qty_std_robust'].  \n",
    "  - Comment on what went wrong.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[3 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Robust normalization vs. Z-normalization**<br/>\n",
    "\n",
    "> Robust appears to properly consider out-sized outliers and attempt to eliminate their influence in the rescaling of the data. I like how SciKit Learn called it \"zoomed in\" in their example plot, as it seems to be a method of looking at the majority of the data. Z-normalization will scale the data set down but keep the outsized outliers, thus creating the same plotting problem despite the new scale.\n",
    "\n",
    "**Robust normalization is more robust than Z-normalization**<br/>\n",
    "\n",
    "> I'm not familiar with how the word robust is being applied here, but I'll take a stab at explaining it without that word.\n",
    "\n",
    "> Robust Normalization focuses on the middle of the dataset, the inter-quartile range, which means that anything that lies above the 75th percentile and below the 25th percentile doesn't factor into how the data is normalized _as heavily_ as it does with Z-score normalizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import robust_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Code here:\n",
    "churn_processed['qty_std_robust'] = robust_scale(churn.quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare minimum, maximum, mean, standard deviation, and the median of churn['quantity'] with churn_processed['qty_std_robust']\n",
    "# Add code here\n",
    "compare = pd.DataFrame([churn['quantity'].describe(), churn_processed['qty_std_robust'].describe()])\n",
    "compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Failure of Robust Normalization**<br/>\n",
    "\n",
    "> Robust normalizing failed on the `quantity` data because there is not inter-quartile range on the data due to the density of values of 1. See the describe results below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Rescale `quantity` using Z-normalization, but normalize `quantity` **per user**, i.e. group by `user_id` so that the mean and standard deviation computed to normalize are computed separately by each `user_id`. Call the rescaled feature `qty_std_Z_byuser`. Present a histogram of `qty_std_Z_byuser`.  Briefly describe why and when you think this kind of normalization makes sense.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[3 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-normalize quantity per user_id\n",
    "# Add code here\n",
    "churn_processed['qty_std_Z_byuser'] = churn.groupby('user_id', group_keys=False)['quantity'].apply(lambda x: (x - x.mean()/x.std()) if x.std() > 0 else x)\n",
    "churn_processed['qty_std_Z_byuser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_processed['qty_std_Z_byuser'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present histogram of qty_std_Z_byuser\n",
    "# Add code here\n",
    "churn_processed['qty_std_Z_byuser'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.quantity.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What could be the purpose of this normalization?**\n",
    "\n",
    "> I'm unsure if this is useful without dropping the quite extreme outlier of 1200. Almost no other quantity comes even close. \n",
    "\n",
    "> That said, this is a useful approach to try to standardize between users and identify some variability between users buying habits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Convert `item_id` into a category column in `churn_processed`.  Replace the `item_id` of all the items sold only once in the entire data with `\"999999\"`.  How many item ids are of category `\"999999\"`?  Display 10 rows of `churn_processed` where `item_id` is category `\"999999\"`.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert item_id into a category column in churn_processed\n",
    "# Add code here\n",
    "churn_processed['item_id'] = churn['item_id'].astype('category')\n",
    "churn_processed.select_dtypes('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_processed.item_id.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n",
    "# Add Category\n",
    "churn_processed.item_id = churn_processed.item_id.cat.add_categories(999999)\n",
    "#  Replace the item_id of all the items sold only once in the entire data with \"999999\"   \n",
    "for item in (churn_processed.item_id.value_counts() == 1).items():\n",
    "    if item[1]:\n",
    "        churn_processed.loc[churn_processed.item_id == item[0],'item_id'] = 999999\n",
    "# How many item ids are of category \"999999\"\n",
    "churn_processed.item_id.value_counts()[999999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 rows of churn_processed where item_id is category 999999\n",
    "churn_processed[churn_processed['item_id'] == 999999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
