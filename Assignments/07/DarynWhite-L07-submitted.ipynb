{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment `L07`\n",
    "##### Daryn White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "import numpy\n",
    "import sklearn\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. use pandas read_csv with sep='\\t' to read in the following 2 files available from the us naval academy: <span style=\"color:red\" float:right>[1 point]</span>\n",
    "   - url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "   - url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_tweets = pandas.read_csv('https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt', sep='\\t', header=0, names=[\"sent\",\"tweet\"])\n",
    "gen_tweets = pandas.read_csv('https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt', sep='\\t', header=0, names=[\"sent\",\"tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. concatenate these 2 data sets into a single data frame called LabeledTweets that has 2 columns, named Sentiment and Tweet <span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledTweets = pandas.concat([key_tweets,gen_tweets])\n",
    "LabeledTweets.rename(columns={'sent':\"Sentiment\",'tweet':\"Tweet\"},inplace=True)\n",
    "LabeledTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. replace sentiment labels `{'POLIT': 1, 'NOT': 0}` <span style=\"color:red\" float:right>[0 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledTweets.loc[LabeledTweets[\"Sentiment\"] == \"POLIT\", \"Sentiment\"] = 1\n",
    "LabeledTweets.loc[LabeledTweets[\"Sentiment\"] == \"NOT\", \"Sentiment\"] = 0\n",
    "LabeledTweets['Sentiment'] = LabeledTweets['Sentiment'].astype('int',copy=False)\n",
    "LabeledTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. clean the tweets <span style=\"color:red\" float:right>[9 points]</span>\n",
    "   1. remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "   2. remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "   3. **replace** (not remove) all punctuation marks with a space (\" \")\n",
    "   4. **replace** all numbers with a space\n",
    "   5. **replace** all non ascii characters with a space\n",
    "   7. convert all characters to lowercase\n",
    "   8. strip extra whitespaces\n",
    "   9. lemmatize tokens\n",
    "   9. No need to remove stopwords because TfidfVectorizer will take care of that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_tweets(text, steps):\n",
    "    for step in steps:\n",
    "        if step == 'remove_handles':\n",
    "            text = re.sub(r\"@\\w{1,}\",\"\",text)\n",
    "        elif step == 'remove_links':\n",
    "            text = re.sub(r\"[htps]{4,5}\\:\\/\\/[.\\/\\-\\w]{1,}\\.[a-z]{2,3}\",\"\",text)\n",
    "        elif step == 'repl_punct':\n",
    "            text = re.sub(r\"[!\\\"#$%&\\'()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~]{1,}\",\" \",text)\n",
    "        elif step == 'repl_numb':\n",
    "            text = re.sub(r\"\\d{1,}\",\" \", text)\n",
    "        elif step == 'repl_nonascii':\n",
    "            text = str().join([c for c in text if ord(c) < 128])\n",
    "        elif step == 'lower_all':\n",
    "            text = text.lower()\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = \" \".join([w for w in text.split()])\n",
    "        elif step == 'lemmatize':\n",
    "            text = \" \".join([nltk.stem.WordNetLemmatizer().lemmatize(w) for w in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['remove_handles','remove_links','repl_punct','repl_numb','repl_nonascii','lower_all','strip_whitespace','lemmatize']\n",
    "LabeledTweets[\"Cleaned_Tweet\"] = LabeledTweets[\"Tweet\"].map(lambda s: cleaning_tweets(s, steps))\n",
    "LabeledTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use `TfidfVectorizer` from `sklearn` to prepare the data for machine learning. Use max_features = 50.  <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize(data, max_feat = 50):\n",
    "    if not isinstance(data,pandas.Series):\n",
    "        raise TypeError(\"Data need to be in a Series format\")\n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_features=max_feat, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(data)\n",
    "    tfidf_df = pandas.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return tfidf_matrix, tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifty_feats, fifty_feats_df = tfidf_vectorize(LabeledTweets[\"Cleaned_Tweet\"])\n",
    "print(fifty_feats_df.shape[0])\n",
    "fifty_feats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use `sklearn` `LogisticRegression` to train a model on the results on 75% of the data. <span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matrix,array):\n",
    "    return sklearn.model_selection.train_test_split(matrix,array,test_size=round(array.size*0.25))\n",
    "\n",
    "def logistic_regression(x_train,y_train):\n",
    "    return sklearn.linear_model.LogisticRegression().fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn, x_tst, y_trn, y_tst = split_data(fifty_feats,LabeledTweets['Sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fifty = logistic_regression(x_trn,y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. determine the accuracy on the training data and the test data.   Determine the baseline accuracy. <span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Training accuracy: {numpy.mean(y_trn == lr_fifty.predict(x_trn))}\n",
    "Testing accuracy: {numpy.mean(y_tst == lr_fifty.predict(x_tst))}\n",
    "Baseline accuracy: {numpy.max([numpy.mean(y_tst == 1),numpy.mean(y_tst == 0)])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Repeat steps 5, 6, and 7  with TfidfVectorizer max_features set to 5, 500, 5000, 50000 and discuss your accuracies. <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_feats, five_feats_df = tfidf_vectorize(LabeledTweets[\"Cleaned_Tweet\"], max_feat=5)\n",
    "xtrain,xtest,ytrain,ytest = split_data(five_feats,LabeledTweets.Sentiment.values)\n",
    "lr_five = logistic_regression(xtrain,ytrain)\n",
    "print(f\"\"\"\n",
    "Training accuracy: {numpy.mean(ytrain == lr_five.predict(xtrain))}\n",
    "Testing accuracy: {numpy.mean(ytest == lr_five.predict(xtest))}\n",
    "Baseline accuracy: {numpy.max([numpy.mean(ytest == 1),numpy.mean(ytest == 0)])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 500 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_hund_feats, five_hund_feats_df = tfidf_vectorize(LabeledTweets[\"Cleaned_Tweet\"], max_feat=500)\n",
    "xtrain,xtest,ytrain,ytest = split_data(five_hund_feats,LabeledTweets.Sentiment.values)\n",
    "lr_five_hund = logistic_regression(xtrain,ytrain)\n",
    "print(f\"\"\"\n",
    "Training accuracy: {numpy.mean(ytrain == lr_five_hund.predict(xtrain))}\n",
    "Testing accuracy: {numpy.mean(ytest == lr_five_hund.predict(xtest))}\n",
    "Baseline accuracy: {numpy.max([numpy.mean(ytest == 1),numpy.mean(ytest == 0)])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_thou_feats, five_thou_feats_df = tfidf_vectorize(LabeledTweets[\"Cleaned_Tweet\"], max_feat=5000)\n",
    "xtrain,xtest,ytrain,ytest = split_data(five_thou_feats,LabeledTweets.Sentiment.values)\n",
    "lr_five_thou = logistic_regression(xtrain,ytrain)\n",
    "print(f\"\"\"\n",
    "Training accuracy: {numpy.mean(ytrain == lr_five_thou.predict(xtrain))}\n",
    "Testing accuracy: {numpy.mean(ytest == lr_five_thou.predict(xtest))}\n",
    "Baseline accuracy: {numpy.max([numpy.mean(ytest == 1),numpy.mean(ytest == 0)])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 50,000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifty_thou_feats, fifty_thou_feats_df = tfidf_vectorize(LabeledTweets[\"Cleaned_Tweet\"], max_feat=50000)\n",
    "xtrain,xtest,ytrain,ytest = split_data(fifty_thou_feats,LabeledTweets.Sentiment.values)\n",
    "lr_fifty_thou = logistic_regression(xtrain,ytrain)\n",
    "print(f\"\"\"\n",
    "Training accuracy: {numpy.mean(ytrain == lr_fifty_thou.predict(xtrain))}\n",
    "Testing accuracy: {numpy.mean(ytest == lr_fifty_thou.predict(xtest))}\n",
    "Baseline accuracy: {numpy.max([numpy.mean(ytest == 1),numpy.mean(ytest == 0)])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "> Based on these tests above we seem to have peak accuracy at 500 features, with the increases in magnitude after only decreasing the accuracy. \n",
    ">\n",
    "> I believe this is a simplistic example of over-fitting an algorithm. It's possible that we'll find slightly increased accuracy if we move up or down around 500 features in smaller increments, which would be an example of algorithm optimization. Of course, this is a relatively small dataset to work with but proves the point that over-fit is something easy to do if one doesn't pay close attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
