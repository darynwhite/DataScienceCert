{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7240e7c",
   "metadata": {},
   "source": [
    "# Assignment (Lesson 05)\n",
    "# Data Preparation and Feature Selection\n",
    "Steps in a data science project\n",
    "1. Acquire data\n",
    "2. Exploratory Data analysis (EDA)\n",
    "3. Data Processing\n",
    "    1. Data Preparation\n",
    "    2. Feature Selection\n",
    "4. Predictive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9c640",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "Python, like most programming languages, has pre-made software methods.  These pre-made software methods are organized and combined by topic into packages.  The packages that we want are:\n",
    "- numpy (numerical python)\n",
    "- pandas (panel data aka tables)\n",
    "- sklearn (sci-kit learn for predictive analytics)\n",
    "- matplotlib (data plotting for matrix-like data)  \n",
    "\n",
    "We need to \"import\" these packages so that we can use their methods in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a30df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "# Allow inline plotting in Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e6edfd",
   "metadata": {},
   "source": [
    "## Data Preparation on the Mammographic Masses Dataset (Mamm)\n",
    "### Acquire data\n",
    "We will get our data from the University of California, Irvine Machine Learning Repository.  Our dataset was used to determine the effectivity of radiological evaluations of breast cancer diagnoses in women who have breast tumors.  You can get some information on the data from here:  http://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file:\n",
    "url = \"../data/mammographic_masses.data\"\n",
    "# Alternate data source:\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\"\n",
    "\n",
    "# Download the data\n",
    "Mamm = pd.read_csv(url, header=None)\n",
    "\n",
    "# Replace the default column names (0, 1, 2, 3, 4, 5) with meaningful names\n",
    "Mamm.columns = [\"BI_RADS\", \"Age\", \"Shape\", \"Margin\", \"Density\", \"Severity\"]\n",
    "\n",
    "Mamm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a29d8",
   "metadata": {},
   "source": [
    "### Some preliminary EDA:\n",
    "\"BI_RADS\", and \"Density\" are ordinal columns.  We will assume that they are numeric.  \n",
    "\"Age\" and \"Severity\" are numeric columns.    \n",
    "\"Shape\" and \"Margin\" are category columns but they are encoded as integers.  \n",
    "\n",
    "Show the actual data types of these columns.  Can you guess why the data types of these 5 columns are `object`?\n",
    "\n",
    "<span style=\"color:red\" float:right>[0 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80988bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n",
    "Mamm.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18115297",
   "metadata": {},
   "source": [
    "> There's probably NaNs or bad data in the columns that are `object`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738bba83",
   "metadata": {},
   "source": [
    "### Some Data Processing\n",
    "In the following sections you will do the following to the Mamm dataframe:\n",
    "- Replace unusable entries with null/nan  \n",
    "- Change types of data.\n",
    "- Correct unexpected values (outliers)\n",
    "- decode category data    \n",
    "- Consolidate categories in category data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18542b4",
   "metadata": {},
   "source": [
    "#### Replace Missing Values with Nulls\n",
    "Coerce all columns, even category columns, that contain missing values to numeric data using `pd.to_numeric`.  You might get an error, like `Unable to parse string`.  You need to tell `pd.to_numeric` that it should **coerce** the casting when it encounters a value that it cannot parse.  The category columns in this dataset are encoded as integers.  We will make use of that encoding.  Any non-numeric value will be replaced with a nan and you will get nans for missing numeric and category values.  After you replace all the non-numeric values, present the first five rows with `Mamm.head()`.\n",
    "\n",
    "<span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056de095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce all the data to numeric data\n",
    "# Coercion will introduce nans/nulls for the non-numeric values in all columns\n",
    "# Because the categories are encoded as integers, the missing categories will also be nans/nulls after coercion.\n",
    "# Add code here\n",
    "for col in Mamm.dtypes.items():\n",
    "    if col[1] == 'object':\n",
    "        Mamm[col[0]] = pd.to_numeric(Mamm[col[0]],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92483d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b03980",
   "metadata": {},
   "source": [
    "#### Replace Outliers\n",
    "Values that are obviously incorrect are often replaced with averages.  Often, outlier replacements with averages are inappropriate because the extreme values have some meaning.  For instance, from the data dictionary we know that BI_RADS should range from 1 to 5.  BI_RADS values beyond 1 and 5 were added by physicians who did not adhere to the accepted range.  In this case, BI_RADS greater than 5 should be \"clipped\" at 5 and BI_RADS less than 1 should be \"clipped\" at 1. \n",
    "\n",
    "<span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap BI_RADS values to a range of 1 to 5\n",
    "# Add code here\n",
    "Mamm.loc[Mamm['BI_RADS'] > 5, 'BI_RADS'] = 5\n",
    "Mamm.loc[Mamm['BI_RADS'] < 1, 'BI_RADS'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d30474",
   "metadata": {},
   "source": [
    "### Consolidate and decode category columns\n",
    "\n",
    "Decoding a category is when categories are coded as numbers and we replace those numbers with actual categories.  \n",
    "Consolidating (aka binning or grouping) of categories is means that multiple categories are renamed to a single category.  \n",
    "The decoding and consolidating of categories can occur at the same time.  \n",
    "\n",
    "- Shape\n",
    "  - The original category codes are: round=1; oval=2; lobular=3; irregular=4;  \n",
    "  - The proper consolidated category decoding is: 1 $\\rightarrow$ oval; 2 $\\rightarrow$ oval; 3 $\\rightarrow$ lobular; 4 $\\rightarrow$ irregular;  \n",
    "- Margin\n",
    "  - The orginal category codes are: circumscribed=1; microlobulated=2; obscured=3; ill-defined=4; spiculated=5  \n",
    "  - The proper consolidated category decodes are: 1 $\\rightarrow$ circumscribed; 2 $\\rightarrow$ ill-defined; 3 $\\rightarrow$ ill-defined; 4 $\\rightarrow$ ill-defined; 5 $\\rightarrow$ spiculated;\n",
    "\n",
    "After you decode and consolidate, present the first five rows with `Mamm.head()`. \n",
    "\n",
    "<span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4252a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The category columns are decoded and categories are consolidated\n",
    "\n",
    "# The Shape variable is decoded as follows:  1 and 2 to oval;  3 to lobular; 4 to irregular\n",
    "# Add code here\n",
    "mapper = {1:\"oval\", 2:\"oval\", 3:\"lobular\", 4:\"irregular\"}\n",
    "Mamm.replace({'Shape':mapper}, inplace=True)\n",
    "# The Margin variable is decoded as follows:  1 to circumscribed;  2, 3, 4 to ill_defined; 5 to spiculated\n",
    "# Add code here\n",
    "mapper = {1:\"circumscribed\",2:\"ill-defined\",3:\"ill-defined\",4:\"ill-defined\",5:\"spiculated\"}\n",
    "Mamm.replace({'Margin':mapper}, inplace=True)\n",
    "\n",
    "#####\n",
    "#> Converting to categories for later\n",
    "Mamm[['Shape','Margin']] = Mamm[['Shape','Margin']].astype('category')\n",
    "\n",
    "# Present the first few rows\n",
    "# Add code here\n",
    "Mamm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944a619",
   "metadata": {},
   "source": [
    "> I choose to show the first 5 & last 5 via the call/print method in a notebook so I can more readily detect any drift or oddities I create. Not very applicabable here but is useful to me when working with the larger datasets at work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859ba29",
   "metadata": {},
   "source": [
    "### Some More EDA\n",
    "- Show the shape of the dataframe\n",
    "- Use the `pandas` `isna` method to show the distribution of nulls among the columns.\n",
    "  \n",
    "<span style=\"color:red\" float:right>[0 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18641b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shape of the data frame\n",
    "# Add code here\n",
    "Mamm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b393901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of nulls among the columns\n",
    "# Add code here\n",
    "Mamm.isna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e17c12",
   "metadata": {},
   "source": [
    "### Drop Rows with Multiple Missing Values\n",
    "When a row has too many missing values, then it should not be used.  We can stipulate a threshold requirement of available values per row.  We will require that each row contains at least 5 values.  This requirement means that no row is allowed more than 1 missing value.  \n",
    "Remove the rows that have more than one missing value.  \n",
    "- Use the `pandas` `dropna` method and set the `thresh` argument.  \n",
    "- Show the shape of the dataframe after you drop the rows with multiple nulls. \n",
    "- Use the `pandas` `isna` method to show the number of nulls per column after dropping rows with multiple nulls\n",
    "\n",
    "<span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e664f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows\n",
    "# Add code here\n",
    "Mamm.dropna(thresh=5, inplace=True)\n",
    "\n",
    "# Show the shape of the data frame\n",
    "# Add code here\n",
    "Mamm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of nulls among the columns\n",
    "# Add code here\n",
    "Mamm.isna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11c129",
   "metadata": {},
   "source": [
    "## Impute Missing Values\n",
    "Use the median values to impute missing values for true numerical columns (`Age`, `BI_RADS`, `Density`).  `Margin` and `Shape` originally looked numeric, but they are categorical.  Therefore, do not use median on `Margin` and `Shape`.  \n",
    "\n",
    "### Determine the imputation values for Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Mamm.columns:\n",
    "    if pd.api.types.is_numeric_dtype(Mamm[col]):\n",
    "        print((f\"The column is {col} with medians\\n\\tpd.Seris.median() : {Mamm[col].median()} | numpy.nanmediam : {np.nanmedian(Mamm[col].values)}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53600915",
   "metadata": {},
   "source": [
    "> It appears, based on this dataset, that the `pandas.Series.median()` and `numpy.nanmediam()` methods are effectively equal. I'll lean on Pnadas only from here.\n",
    "\n",
    "> Also, I'll loop over the dataframe rather than explicityly call out columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e92e7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```python\n",
    "# Replace missing age values with the median \n",
    "MedianAge = np.nanmedian(Mamm.loc[:,\"Age\"])\n",
    "HasNanAge = pd.isnull(Mamm.loc[:,\"Age\"])\n",
    "print('Now we replace', HasNanAge.sum(),'missing age values with the age median (', MedianAge, ')')\n",
    "Mamm.loc[HasNanAge, \"Age\"] = MedianAge\n",
    "Mamm.isna().sum(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056e2c8",
   "metadata": {},
   "source": [
    "### Impute Missing values for BI_RADS and Density\n",
    "Assign the column medians to the null values in the respective numeric columns.\n",
    "- Use the `pandas` `isnull` method to identify the nulls\n",
    "- Use the `numpy` `nanmedian` to determine the median for imputation\n",
    "- Use the `pandas` `isna` method to show the number of nulls per column after the imputation   \n",
    "  \n",
    "<span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1287fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm.isna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c977d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Mamm.columns:\n",
    "    if pd.api.types.is_numeric_dtype(Mamm[col]):\n",
    "        Mamm.loc[pd.isna(Mamm[col]), col] = Mamm[col].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1936994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm.isna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff743b54",
   "metadata": {},
   "source": [
    "### Replace missing values for the two categorical columns\n",
    "- Use `pandas` `value_counts()` method to determine the distribution of categories in `Shape` and `Margin` before imputation.\n",
    "- Use `pandas` `isnull()` method to identify the missing values\n",
    "- Assign the most common value to the null values in the respective categorical columns. \n",
    "- After the imputation, use the `pandas` `isna` method to show the number of nulls after the imputation.\n",
    "- Use `pandas` `value_counts()` method to determine the distribution of categories after imputation.\n",
    "\n",
    "<span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6579440",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm['Shape'].value_counts(dropna=False), Mamm['Margin'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ac37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Mamm.dtypes.items():\n",
    "    if col[1] == 'category':\n",
    "        Mamm.loc[Mamm[col[0]].isna(),col[0]] = Mamm[col[0]].mode().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm['Shape'].value_counts(dropna=False), Mamm['Margin'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85baf8",
   "metadata": {},
   "source": [
    "### One hot encode the categorical variables\n",
    "- Use `OneHotEncoder` from `sklearn.preprocessing` to one-hot encode the two categorical variables, `Shape` and `Margin`.\n",
    "- Make sure that the new columns have descriptive hybrid names by using the `get_feature_names_out` method.\n",
    "- Add the new binary columns to the dataframe.\n",
    "- drop the original columns, `Shape` and `Margin`\n",
    "- Show the first few rows of the dataframe.\n",
    "\n",
    "<span style=\"color:red\" float:right>[3 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b15a35",
   "metadata": {},
   "source": [
    "> Review what the dataframe currently looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7851dda",
   "metadata": {},
   "source": [
    "> Start the OneHotEncoding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df8632",
   "metadata": {},
   "source": [
    "> Ok... package is in. Now to create the object in memory, encode the category columns (Shape, Margin), add those to the dataframe, drop the original columns, and show the tranformation process. Fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse_output=False)\n",
    "encode_cols = Mamm.select_dtypes('category').copy()\n",
    "onehot.fit(encode_cols)\n",
    "print(f\"The new column names are: {onehot.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm[onehot.get_feature_names_out()] = onehot.transform(encode_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8782ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0db1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm = Mamm.drop(['Shape','Margin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdda276",
   "metadata": {},
   "source": [
    "## End of Data Preparation on the Mammographic Masses Dataset (Mamm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085acda4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Feature Selection on the Indian  Liver Patient Dataset (ILPD)\n",
    "Feature selection is a process of removing features that are redundant and that could lead to overfitting, singular matrices, and other problems associated with high cardinality (Curse of dimensionality:  https://en.wikipedia.org/wiki/Curse_of_dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c748d",
   "metadata": {},
   "source": [
    "### Acquire Data\n",
    "\n",
    "We will get our data from the University of California, Irvine Machine Learning Repository. Our dataset was used to determine if blood test data could be sufficient to identify liver disease in rural areas with few physicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file:\n",
    "url = \"../data/Indian Liver Patient Dataset (ILPD).csv\"\n",
    "# Alternate data source:\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian Liver Patient Dataset (ILPD).csv\"\n",
    "url = url.replace(\" \", \"%20\")\n",
    "\n",
    "# Download the data\n",
    "ILPD = pd.read_csv(url, header=None)\n",
    "\n",
    "# Replace the default column names (0, 1, 2, 3, 4, 5) with meaningful names\n",
    "ILPD.columns = [\"Age\",\"Gender\",\"DB\",\"TB\",\"Alkphos\",\"Sgpt\",\"Sgot\",\"TPr\",\"ALB\",\"AGRatio\",\"Selector\"]\n",
    "\n",
    "ILPD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f483ac1",
   "metadata": {},
   "source": [
    "### Data Preparation for ILPD\n",
    "- All columns should be numeric and continuous\n",
    "    - Remove binary columns (numeric and categorical) because their mutual information scores will be lower\n",
    "    - Remove any categorical columns\n",
    "- Remove or impute any missing values\n",
    "\n",
    "<span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e55670",
   "metadata": {},
   "outputs": [],
   "source": [
    "ILPD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ddf0bd",
   "metadata": {},
   "source": [
    "> I'll loop over the entire dataframe and run a test against the quantity of unique values to drop the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ILPD.columns:\n",
    "    if ILPD[col].astype('category').describe()['unique'] == 2:\n",
    "        ILPD = ILPD.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ILPD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66f813",
   "metadata": {},
   "source": [
    "> It appears that only the AGRatio column contans NaNs, so I'll impute that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ILPD.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values or remove rows with nulls\n",
    "# Add Code here\n",
    "for col in ILPD.columns:\n",
    "    if pd.api.types.is_numeric_dtype(ILPD[col]) and ILPD[col].isna().any():\n",
    "        ILPD.loc[pd.isna(ILPD[col]), col] = ILPD[col].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ILPD.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0db3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ILPD.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e49738",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "https://en.wikipedia.org/wiki/Mutual_information\n",
    "Below is a wrapper for determining the mutual information between two continuous (numeric) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a181dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "# x is the first input variable\n",
    "# y is the second input variable\n",
    "# bins is the number of discretized values that will be used for the two input variables\n",
    "def calc_MI(x, y, bins=80):\n",
    "    if (bins > 1):\n",
    "        c_xy = np.histogram2d(x, y, bins)[0]\n",
    "        mi = mutual_info_score(None, None, contingency = c_xy)\n",
    "    else:\n",
    "        mi = mutual_info_score(x, y)\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999c8c2",
   "metadata": {},
   "source": [
    "### Create method to list  all column pairs together with their mutual information score\n",
    "Write a method called `listMutualInformationScores`.  It uses the above method (`calc_MI`) in a loop to find the mutual information between all possible pairs of coulmns in the data.  The input to the function is the dataframe of continuous variables, specifically the prepared ILPD dataset.  \n",
    "\n",
    "The method returns a list of lists.  Each inner list contains three items:  the x-column, the y-column, and the mutual information score. The list of lists contains every possible pair of coulmns in the data.  The result should have a form similar to the following, except that the outer list is much longer and contains all possible column pairs:  \n",
    "`[['Alkphos', 'Sgot', 0.33],\n",
    "['Sgot', 'AGRatio', 0.23],\n",
    "['Age', 'Sgot', 0.35],\n",
    "['Sgpt', 'AGRatio', 0.30],\n",
    "['Sgot', 'ALB', 0.29],\n",
    "['Sgot', 'TPr', 0.33]]`\n",
    "\n",
    "<span style=\"color:red\" float:right>[3 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50586a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the method listMutualInformationScores\n",
    "\n",
    "def listMutualInformationScores(df):\n",
    "    \"Something about the function\"\n",
    "    \n",
    "    output = []\n",
    "    for n,col in enumerate(df.columns):\n",
    "        for c in df.columns[n:]:\n",
    "            if col == c:\n",
    "                continue\n",
    "            else:\n",
    "                output.append((col, c, calc_MI(ILPD[col],ILPD[c])))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the method listMutualInformationScores\n",
    "listMutualInformationScores(ILPD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ae88d",
   "metadata": {},
   "source": [
    "### Present the mutual information results\n",
    "- Package the output into a dataframe\n",
    "- Sort the rows in descending order of mutual information\n",
    "- Present the dataframe\n",
    "\n",
    "The first column could be x, the second column could be called y and the third column could be called mi.  x and y are the pair of columns pair and mi is the pair's mutual information score.  The result should have a form similar to the following:\n",
    "\n",
    "| x | y | mi |\n",
    "| --- | --- | --- |\n",
    "| Age | Sgot | 0.35 |\n",
    "| Sgot | TPr | 0.33 |\n",
    "| Alkphos | Sgot | 0.33 |\n",
    "| Sgpt | AGRatio | 0.30 |\n",
    "| Sgot | ALB | 0.29 |\n",
    "| Sgot | AGRatio | 0.23 |\n",
    "\n",
    "<span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present the results as dataframe\n",
    "ILPD_mutual = pd.DataFrame(listMutualInformationScores(ILPD), columns=['x','y','mi'])\n",
    "ILPD_mutual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a86361",
   "metadata": {},
   "source": [
    "### Discussion on Mutual Information in ILPD\n",
    "Lets assume a threshold of 1 for the mutual information score\n",
    "\n",
    "Which columns would you eliminate? Why?  To answer these questions, you may need to read-up on feature selection with mutual information score.\n",
    "\n",
    "<span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07792d",
   "metadata": {},
   "source": [
    "> In reading a little about what Mutual Information (or Information Gain) is from https://machinelearningmastery.com/information-gain-and-mutual-information/, I believe I can say the following:\n",
    ">\n",
    "> Setting the threshold at 1 is saying that any score above one and those to columns/variables are sufficiently dependent to use going forward but the columns below 1 are too independent and thus too unpredictable to continue into the construction of a model. \n",
    ">\n",
    "> Based on this idea and the code below, that leaves ['Alkphos', 'Sgpt', 'Sgot'] as independent variables that are probably too unpredictable to be effectively modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ILPD.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "ILPD_mutual[ILPD_mutual['mi'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = []\n",
    "for row in ILPD_mutual[ILPD_mutual['mi'] > 1].iterrows():\n",
    "    if row[1]['x'] not in feature_cols:\n",
    "        feature_cols.append(row[1]['x'])\n",
    "    if row[1]['y'] not in feature_cols:\n",
    "        feature_cols.append(row[1]['y'])\n",
    "        \n",
    "not_features = [col for col in ILPD.columns.to_list() if col not in feature_cols]\n",
    "\n",
    "\n",
    "feature_cols, not_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
