{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "We build on the feature engineering we did in the last assignment and run k-means on the data with RFM features in order to do **customer segmentation**. Since k-means is unsupervised, we will also encounter challenges around interpreting results at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "col_names = ['user_id', 'gender', 'address', 'store_id', 'trans_id', 'timestamp', 'item_id', 'quantity', 'dollar']\n",
    "churn = pd.read_csv(\"../../data/retail-churn.csv\", sep = \",\", skiprows = 1, names = col_names)\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the feature engineering steps on the data to extract RFM features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn['date'] =  pd.to_datetime(pd.to_datetime(churn['timestamp'], format = '%m/%d/%Y %H:%M').dt.date)\n",
    "churn_agg = churn.groupby(['user_id', 'date']).agg({'dollar': 'sum', 'quantity': 'sum'})\n",
    "churn_agg = churn_agg.reset_index()\n",
    "churn_roll = pd.DataFrame()\n",
    "churn_roll['dollar_roll_sum_7D'] = churn_agg.groupby('user_id').rolling(window = '7D', on = 'date')['dollar'].sum()\n",
    "churn_roll['quantity_roll_sum_7D'] = churn_agg.groupby('user_id').rolling(window = '7D', on = 'date')['quantity'].sum()\n",
    "churn_roll = churn_roll.reset_index()\n",
    "churn_roll['last_visit_ndays'] = churn_agg.groupby('user_id')['date'].diff(periods = 1).dt.days\n",
    "print(churn_roll.shape)\n",
    "\n",
    "# Should we impute or drop NaN/NaT in churn_roll['last_visit_ndays']?\n",
    "imputation_value = churn_roll['last_visit_ndays'].max() # None # \n",
    "if imputation_value is None:\n",
    "    # Drop (Remove all rows with NaN):\n",
    "    churn_roll.dropna(inplace = True)\n",
    "    print(churn_roll.shape)\n",
    "else:\n",
    "    # Impute (Replace all NaN in last_visit_ndays):\n",
    "    churn_roll['last_visit_ndays'] = churn_roll['last_visit_ndays'].fillna(imputation_value)\n",
    "\n",
    "churn_roll.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFM features are:  'dollar_roll_sum_7D', 'quantity_roll_sum_7D', 'last_visit_ndays'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def _run_kmeans(dataframe,cols,cluster,init='auto',rand_state=41):\n",
    "    menas = KMeans(n_clusters=cluster,n_init=init,random_state=rand_state)\n",
    "    menas.fit(dataframe[cols])\n",
    "    return menas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a k-means algorithm on the 3 normalized RFM features using $k = 10$.\n",
    "\n",
    "- What are the cluster centroids? The cluster centroids should be reported in the **original scale**, not the normalized scale. <span style=\"color:red\" float:right>[2 point]</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['dollar_roll_sum_7D','quantity_roll_sum_7D','last_visit_ndays']\n",
    "churn_roll_scaled = churn_roll[cols].apply(lambda x: (x - x.mean()) / x.std(), axis = 0)\n",
    "means = _run_kmeans(churn_roll_scaled,cols,cluster=10)\n",
    "centroids = pd.DataFrame(means.cluster_centers_, columns=cols)\n",
    "(centroids * churn_roll[cols].std() + churn_roll[cols].mean()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Our earlier choice of $k=10$ was arbitrary. To find a better number of $k$ create a **scree plot**, which plots the number of clusters $k$ on the x-axis and the sum of squared distances from each point to its cluster centroid on the y-axis. We can get the latter by calling the `inertia_` attribute as shown in the lab. Plot the scree plot for $k$ values from 1 to 15. <span style=\"color:red\" float:right>[4 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as plt\n",
    "\n",
    "\n",
    "data_cap = []\n",
    "for n in range(1,16):\n",
    "    means = _run_kmeans(churn_roll_scaled,cols,cluster=n)\n",
    "    data_cap.append((n,means.inertia_))\n",
    "\n",
    "k_testing = pd.DataFrame(data_cap, columns=['k','inertia'])\n",
    "plt.line(k_testing,x='k',y='inertia',markers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Based on the scree plot, what is a good value to pick for $k$? Provide a brief justification for your choice. <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given the RFM dataset we are working with, I would say that 8 is the minimum number of `k` clusters, but 14 might be more optimal\n",
    ">\n",
    "> The jump from 13 to 14 is the last one that is an order of magnitude jump and nothing above it provides the same delta. If 14 becomes unmanageable for any reason, then I believe 8 is the minimum because that is approximately the location where things start to level off before rescaling the plot to rule out clusters sizes of 1 to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train a k-means algorithm on the RFM features using your new value of $k$. Report:\n",
    "- the size (number of items) of each cluster\n",
    "- the mean of each cluster in the original scale\n",
    "- the standard deviation of each cluster in the Z-normalized scale\n",
    "<span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_14 = _run_kmeans(churn_roll_scaled,cols,cluster=14)\n",
    "means_8 = _run_kmeans(churn_roll_scaled,cols,cluster=8)\n",
    "churn_roll['cluster_14'] = means_14.predict(churn_roll_scaled[cols])\n",
    "churn_roll_scaled['cluster_14'] = means_14.predict(churn_roll_scaled[cols])\n",
    "churn_roll['cluster_8'] = means_8.predict(churn_roll_scaled[cols])\n",
    "churn_roll_scaled['cluster_8'] = means_8.predict(churn_roll_scaled[cols])\n",
    "churn_roll['cluster_14'] = churn_roll['cluster_14'].astype('str')\n",
    "churn_roll['cluster_8'] = churn_roll['cluster_8'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grouping by cluster id\")\n",
    "display(churn_roll.value_counts('cluster_14').sort_index(),churn_roll.value_counts('cluster_8').sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(churn_roll.groupby('cluster_14',observed=True)[cols].mean(), churn_roll.groupby('cluster_8',observed=True)[cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(churn_roll_scaled.groupby('cluster_14')[cols].std(), churn_roll_scaled.groupby('cluster_8')[cols].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_14 = plt.scatter_matrix(churn_roll,dimensions=cols,color='cluster_14')\n",
    "fig_14.update_traces(diagonal_visible=False)\n",
    "fig_14.update_layout(height=1000)\n",
    "fig_14.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_8 = plt.scatter_matrix(churn_roll,dimensions=cols,color='cluster_8')\n",
    "fig_8.update_traces(diagonal_visible=False)\n",
    "fig_8.update_layout(height=1000)\n",
    "fig_8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In comparing 14 vs 8 clusters, it appears the 14 may be a bit of an overfit situation. Thus I'm moving forward with only 8 clusters for the final question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Pick 3 clusters at random and describe what makes them different from one another (in terms of their RFM features). <span style=\"color:red\" float:right>[3 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = churn_roll.loc[(churn_roll.cluster_8 == 1) | (churn_roll.cluster_8 == 2) | (churn_roll.cluster_8 == 6)]\n",
    "df_comp.value_counts('cluster_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.scatter_matrix(df_comp,dimensions=cols,color='cluster_8')\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.update_layout(height=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fortunately, these clusters do seem to be genuinely different.\n",
    ">\n",
    "> Cluster 1 appears to be grouped by having the fewest dollars spent and fewest items purchased.\n",
    ">\n",
    "> Cluster 2 appears to buy the highest number of items but not the most expensive ones and not return to the store very often.\n",
    ">\n",
    "> Cluster 6 appears to purchases higher dollar items and fewer quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-sci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
